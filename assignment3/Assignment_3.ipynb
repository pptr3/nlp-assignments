{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4_wqPdlBcKS"
   },
   "source": [
    "# Assignment 3 : Sequence labelling with RNNs\n",
    "In this assignement we will ask you to perform POS tagging.\n",
    "\n",
    "You are asked to follow these steps:\n",
    "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
    "*   Embed the words using GloVe embeddings\n",
    "*   Create a baseline model, using a simple neural architecture\n",
    "*   Experiment doing small modifications to the model\n",
    "*   Evaluate your best model\n",
    "*   Analyze the errors of your model\n",
    "\n",
    "**Corpora**:\n",
    "Ignore the numeric value in the third column, use only the words/symbols and its label.\n",
    "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip \n",
    "\n",
    "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
    "\n",
    "**Baseline**: two layers architecture: a Bidirectional LSTM and a Dense/Fully-Connected layer on top.\n",
    "\n",
    "**Modifications**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and using a CRF in addition to the LSTM. Each of this change must be done by itself (don't mix these modifications).\n",
    "\n",
    "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
    "\n",
    "**Evaluation**: in the end, only the best model of your choice must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech (without considering punctuation classes).\n",
    "\n",
    "**Error Analysis** (optional) : analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
    "\n",
    "**Report**: You are asked to deliver a small report of about 4-5 lines in the .txt file that sums up your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%autosave 1\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "base_dir = 'dependency_treebank/'\n",
    "for filename in os.listdir(base_dir):\n",
    "    with open(base_dir + filename) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [re.sub('\\s+', ' ', x) for x in content]\n",
    "\n",
    "    # remove empty strings\n",
    "    content = [x for x in content if x]\n",
    "    data.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_labels(data):\n",
    "    X = []\n",
    "    y = []\n",
    "    for document in data:\n",
    "        X_i = []\n",
    "        y_i = []\n",
    "        for i in document:\n",
    "            X_i.append(i.split(' ')[0])\n",
    "            y_i.append(i.split(' ')[1])\n",
    "        X.append(X_i)\n",
    "        y.append(y_i)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_features_labels(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X)\n",
    "X_encoded = word_tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(y)\n",
    "y_encoded = tag_tokenizer.texts_to_sequences(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest sentence: 4534\n"
     ]
    }
   ],
   "source": [
    "# check length of longest sentence\n",
    "lengths = [len(seq) for seq in X_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 2300  # sequences greater than 2300 in length will be truncated\n",
    "\n",
    "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "y_padded = pad_sequences(y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_padded, y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199, 2300), (199, 2300))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(model_type, embedding_dimension=50):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library.\n",
    "\n",
    "    :param model_type: name of the word embedding model to load.\n",
    "    :param embedding_dimension: size of the embedding space to consider\n",
    "\n",
    "    :return\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "\n",
    "    download_path = \"\"\n",
    "    '''\n",
    "    # Find the correct embedding model name\n",
    "    if model_type.strip().lower() == 'word2vec':\n",
    "        download_path = \"word2vec-google-news-300\"\n",
    "    '''\n",
    "    if model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove\")\n",
    "\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model\n",
    "\n",
    "\n",
    "# Modify these variables as you wish!\n",
    "# Glove -> 50, 100, 200, 300\n",
    "# Word2Vec -> 300\n",
    "embedding_model_type = \"glove\"\n",
    "embedding_dimension = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "embedding_model = load_embedding_model(embedding_model_type, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6713277101516724),\n",
       " ('princess', 0.5432624220848083),\n",
       " ('throne', 0.5386104583740234),\n",
       " ('monarch', 0.5347574949264526),\n",
       " ('daughter', 0.498025119304657),\n",
       " ('mother', 0.4956442713737488),\n",
       " ('elizabeth', 0.4832652509212494),\n",
       " ('kingdom', 0.47747087478637695),\n",
       " ('prince', 0.4668239951133728),\n",
       " ('wife', 0.4647327661514282)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.most_similar(positive = [\"king\", \"woman\"], negative = [\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use one-hot encoding for output sequences Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Keras' to_categorical function to one-hot encode Y\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tran val test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X[:100, :], y[:100, :, :]\n",
    "X_val, y_val = X[100:150, :], y[100:150, :, :]\n",
    "X_test, y_test = X[150:, :], y[150:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2300),\n",
       " (100, 2300, 46),\n",
       " (50, 2300),\n",
       " (50, 2300, 46),\n",
       " (49, 2300),\n",
       " (49, 2300, 46))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(df):\n",
    "    X, y = split_features_labels(df)\n",
    "    flat_list = [item for sublist in X for item in sublist]\n",
    "    vocab = set(flat_list)\n",
    "    vocab_dict = {} # word to index\n",
    "    current_index = 0\n",
    "    for v in vocab:\n",
    "        vocab_dict[v] = current_index\n",
    "        current_index += 1\n",
    "    \n",
    "    voc_index_to_word = {v: k for k, v in vocab_dict.items()}\n",
    "    \n",
    "    return voc_index_to_word, vocab_dict, np.array(list(vocab_dict.keys()))\n",
    " \n",
    "\n",
    "\n",
    "# Testing\n",
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model, word_listing):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    model_vocab_array = np.array(list(embedding_model.vocab.keys()))\n",
    "    return list(np.setdiff1d(word_listing, model_vocab_array))\n",
    "\n",
    "oov_terms = check_OOV_terms(embedding_model, word_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 2258 (0.28%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), float(len(oov_terms)) / len(word_listing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:37: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (8068, 300)\n"
     ]
    }
   ],
   "source": [
    "def build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, oov_terms, consider_oov=True):\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param oov_terms: list of OOV terms (list)\n",
    "    :param co_occorruence_count_matrix: the co-occurrence count matrix of the given dataset (window size 1)\n",
    "\n",
    "    :return\n",
    "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
    "    \"\"\"\n",
    "    words = np.array(list(word_to_idx.keys()))\n",
    "    embedding_matrix = np.zeros((len(words), embedding_dimension))    \n",
    "    for idx, w in enumerate(words):\n",
    "        if w not in oov_terms:\n",
    "            embedding_matrix[idx, :] = np.array(embedding_model.wv[w])\n",
    "        elif consider_oov == True and idx < len(words) and idx > 0:\n",
    "                # get the first 2 word in the vocab close to the current word index\n",
    "                below_idx = -1\n",
    "                up_idx = -1\n",
    "                model_vocab = list(embedding_model.vocab.keys())\n",
    "\n",
    "                for i in range(idx + 1, len(words)):\n",
    "                    if idx_to_word[i] in model_vocab:\n",
    "                        below_idx = i\n",
    "                        up_idx = i\n",
    "                        break\n",
    "\n",
    "                for k in reversed(range(0, idx - 1)):\n",
    "                    if idx_to_word[k] in model_vocab:\n",
    "                        up_idx = k\n",
    "                        below_idx = k\n",
    "                        break\n",
    "\n",
    "                up_embed = np.array(embedding_model.wv[idx_to_word[up_idx]])\n",
    "                below_embed = np.array(embedding_model.wv[idx_to_word[below_idx]])\n",
    "                mean_oov_word = np.mean((up_embed, below_embed), axis=0)\n",
    "                embedding_matrix[idx, :] = mean_oov_word\n",
    "    if consider_oov == False:\n",
    "        # delete all the cells that have all values zeros, this because these\n",
    "        # rows has not been initialized since we did not condier the oov terms\n",
    "        embedding_matrix =  embedding_matrix[np.all(embedding_matrix != 0, axis=1)]\n",
    "    return embedding_matrix\n",
    "\n",
    "# Testing\n",
    "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, oov_terms)\n",
    "\n",
    "print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
