{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GM9DBN-Qz3k"
   },
   "source": [
    "# Assignment 4\n",
    "\n",
    "**Due to**: TBD\n",
    "\n",
    "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
    "\n",
    "**Summary**: Fact checking, Neural Languange Inference (**NLI**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO_-4CZeRCO7"
   },
   "source": [
    "# Intro\n",
    "\n",
    "This assignment is centred on a particular and emerging NLP task, formally known as **fact checking** (or fake checking). As AI techniques become more and more powerful, reaching amazing results, such as image and text generation, it is more than ever necessary to build tools able to distinguish what is real from what is fake.\n",
    "\n",
    "Here we focus on a small portion of the whole fact checking problem, which aims to determine whether a given statement (fact) conveys a trustworthy information or not. \n",
    "\n",
    "More precisely, given a set of evidences and a fact to verify, we would like our model to correctly predict whether the fact is true or fake.\n",
    "\n",
    "In particular, we will see:\n",
    "\n",
    "*   Dataset preparation (analysis and pre-processing)\n",
    "*   Problem formulation: multi-input binary classification\n",
    "*   Defining an evaluation method\n",
    "*   Simple sentence embedding\n",
    "*   Neural building blocks\n",
    "*   Neural architecture extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGDwg78PS_uy"
   },
   "source": [
    "# The FEVER dataset\n",
    "\n",
    "First of all, we need to choose a dataset. In this assignment we will rely on the [FEVER dataset](https://fever.ai).\n",
    "\n",
    "The dataset is about facts taken from Wikipedia documents that have to be verified. In particular, facts could face manual modifications in order to define fake information or to give different formulations of the same concept.\n",
    "\n",
    "The dataset consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as ```Supported```, ```Refuted``` or ```NotEnoughInfo```. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Oa5FpVpT7p4"
   },
   "source": [
    "## Dataset structure\n",
    "\n",
    "Relevant data is divided into two file types. Information concerning the fact to verify, its verdict and associated supporting/opposing statements are stored in **.jsonl** format. In particular, each JSON element is a python dictionary with the following relevant fields:\n",
    "\n",
    "*    **ID**: ID associated to the fact to verify.\n",
    "\n",
    "*    **Verifiable**: whether the fact has been verified or not: ```VERIFIABLE``` or ```NOT VERIFIABLE```.\n",
    "    \n",
    "*    **Label**: the final verdict on the fact to verify: ```SUPPORTS```, ```REFUTES``` or ```NOT ENOUGH INFO```.\n",
    "    \n",
    "*    **Claim**: the fact to verify.\n",
    "    \n",
    "*    **Evidence**: a nested list of document IDs along with the sentence ID that is associated to the fact to verify. In particular, each list element is a tuple of four elements: the first two are internal annotator IDs that can be safely ignored; the third term is the document ID (called URL) and the last one is the sentence number (ID) in the pointed document to consider.\n",
    "\n",
    "**Some Examples**\n",
    "\n",
    "---\n",
    "\n",
    "**Verifiable**\n",
    "\n",
    "{\"id\": 202314, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"The New Jersey Turnpike has zero shoulders.\", \"evidence\": [[[238335, 240393, \"New_Jersey_Turnpike\", 15]]]}\n",
    "\n",
    "---\n",
    "\n",
    "**Not Verifiable**\n",
    "\n",
    "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nex_8UM4VWuY"
   },
   "source": [
    "## Some simplifications and pre-processing\n",
    "\n",
    "We are only interested in verifiable facts. Thus, we can filter out all non-verifiable claims.\n",
    "\n",
    "Additionally, the current dataset format does not contain all necessary information for our classification purposes. In particular, we need to download Wikipedia documents and replace reported evidence IDs with the corresponding text.\n",
    "\n",
    "Don't worry about that! We are providing you the already pre-processed dataset so that you can concentrate on the classification pipeline (pre-processing, model definition, evaluation and training).\n",
    "\n",
    "You can download the zip file containing all set splits (train, validation and test) of the FEVER dataset by clicking on this [link](https://drive.google.com/file/d/1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1/view?usp=sharing). Alternatively, run the below code cell to automatically download it on this notebook.\n",
    "\n",
    "**Note**: each dataset split is in .csv format. Feel free to inspect the whole dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BspxZcRjW0NG",
    "outputId": "c9bd227a-afc5-4973-ff80-a867d50c7ebd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading FEVER data splits...\n",
      "Download completed!\n",
      "Extracting dataset...\n",
      "Extraction completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_path):\n",
    "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
    "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
    "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(toy_data_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:\n",
    "            response = current_session.get(toy_url,\n",
    "                                   params={'id': toy_data_url_id},\n",
    "                                   stream=True)\n",
    "        save_response_content(response, toy_data_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
    "            loaded_zip.extractall(data_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "download_data('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbH_8errW5MH"
   },
   "source": [
    "# Classification dataset\n",
    "\n",
    "At this point, you should have a reay-to-go dataset! Note that the dataset format changed as well! In particular, we split the evidence set associated to each claim, in order to build (claim, evidence) pairs. The classification label is propagated as well.\n",
    "\n",
    "We'll motivate this decision in the next section!\n",
    "\n",
    "Just for clarity, here's an example of the pre-processed dataset:\n",
    "\n",
    "---\n",
    "\n",
    "**Claim**: \"Wentworth Miller is yet to make his screenwriting debut.\"\n",
    "\n",
    "**Evidence**: \"2\tHe made his screenwriting debut with the 2013 thriller film Stoker .\tStoker\tStoker (film)\"\n",
    "\n",
    "**Label**: Refutes\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: The dataset requires some text cleaning as you may notice!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gH8hIK21Xrl0"
   },
   "source": [
    "# Problem formulation\n",
    "\n",
    "As mentioned at the beginning of the assignment, we are going to formulate the fact checking problem as a binary classification task.\n",
    "\n",
    "In particular, each dataset sample is comprised of:\n",
    "\n",
    "*     A claim to verify\n",
    "*     A set of semantically related statements (evidence set)\n",
    "*     Fact checking label: either evidences support or refute the claim.\n",
    "\n",
    "Handling the evidence set from the point of view of neural models may imply some additional complexity: if the evidence set is comprised of several sentences we might incur in memory problems.\n",
    "\n",
    "To this end, we further simplify the problem by building (claim, evidence) pairs. The fact checking label is propagated as well.\n",
    "\n",
    "Example:\n",
    "\n",
    "     Claim: c1 \n",
    "     Evidence set: [e1, e2, e3]\n",
    "     Label: S (support)\n",
    "\n",
    "--->\n",
    "\n",
    "    (c1, e1, S),\n",
    "    (c1, e2, S),\n",
    "    (c1, e3, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E46flIz_zQy-"
   },
   "source": [
    "## Schema\n",
    "\n",
    "The overall binary classification problem is summed up by the following (simplified) schema\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Wm_YBnFwgJtxcWEBpPbTBEVkpKaL08Jp)\n",
    "\n",
    "Don't worry too much about the **Encoding** block for now. We'll give you some simple guidelines about its definition. For the moment, stick to the binary classification task definition where, in this case, we have 2 inputs: the claim to verify and one of its associated evidences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsiTV-NVdgsF"
   },
   "source": [
    "# Architecture Guidelines\n",
    "\n",
    "There are many neural architectures that follow the above schema. To avoid phenomena like the writer's block, in this section we are going to give you some implementation guidelines.\n",
    "\n",
    "In particular, we would like you to test some implementations so that you explore basic approaches (neural baselines) and use them as building blocks for possible extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJBQm47fe7iE"
   },
   "source": [
    "## Handling multiple inputs\n",
    "\n",
    "The first thing to notice is that we are in a multi-input scenario. In particular, each sample is comprised of a fact and its asssociated evidence statement.\n",
    "\n",
    "Each of these input is encoded as a sequence of tokens. In particular, we will have the following input matrices:\n",
    "\n",
    "*    Claim: [batch_size, max_tokens]\n",
    "*    Evidence: [batch_size, max_tokens]\n",
    "\n",
    "Moreover, after the embedding layer, we'll have:\n",
    "\n",
    "*    Claim: [batch_size, max_tokens, embedding_dim]\n",
    "*    Evidence: [batch_size, max_tokens, embedding_dim]\n",
    "\n",
    "But, we would like to have a 2D input to our classifier, since we have to give an answer at pair level. Therefore, for each sample, we would expect the following input shape to our classification block:\n",
    "\n",
    "*   Classification input shape: [batch_size, dim]\n",
    "\n",
    "**How to do that?**\n",
    "\n",
    "We inherently need to reduce the token sequence to a single representation. This operation is formally known as **sentence embedding**. Indeed, we are trying to compress the information of a whole sequence into a single embedding vector.\n",
    "\n",
    "Here are some simple solutions that we ask you to try out:\n",
    "\n",
    "*   Encode token sequences via a RNN and take the last state as the sentence embedding.\n",
    "\n",
    "*   Encode token sequences via a RNN and average all the output states.\n",
    "\n",
    "*   Encode token sequences via a simple MLP layer. In particular, if your input is a [batch_size, max_tokens, embedding_dim] tensor, the matrix multiplication works on the **max_tokens** dimension, resulting in a [batch_size, embedding_dim] 2D matrix.\n",
    "\n",
    "*   Compute the sentence embedding as the mean of its token embeddings (**bag of vectors**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gypl5z5ElJo1"
   },
   "source": [
    "## Merging multi-inputs\n",
    "\n",
    "At this point, we have to think about **how** we should merge evidence and claim sentence embeddings.\n",
    "\n",
    "For simplicity, we stick to simple merging strategies:\n",
    "\n",
    "*     **Concatenation**: define the classification input as the concatenation of evidence and claim sentence embeddings\n",
    "\n",
    "*     **Sum**: define the classification input as the sum of evidence and claim sentence embeddings\n",
    "\n",
    "*     **Mean**: define the classification input as the mean of evidence and claim sentence embeddings\n",
    "\n",
    "For clarity, if we the sentence embedding of a single input has shape [batch_size, embedding_dim], then the classification input has shape:\n",
    "\n",
    "*     **Concatenation**: [batch_size, 2 * embedding_dim]\n",
    "\n",
    "*     **Sum**: [batch_size, embedding_dim]\n",
    "\n",
    "*     **Mean**: [batch_size, embedding_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhVg9ZLymOUc"
   },
   "source": [
    "# A simple extension\n",
    "\n",
    "Lastly, we ask you to modify previously defined neural architectures by adding an additional feature to the classification input.\n",
    "\n",
    "We would like to see if some similarity information between the claim to verify and one of its associated evidence might be useful to the classification.\n",
    "\n",
    "Compute the cosine similarity metric between the two sentence embeddings and concatenate the result to the classification input.\n",
    "\n",
    "For clarity, since the cosine similarity of two vectors outputs a scalar value, the classification input shape is modified as follows:\n",
    "\n",
    "*     **Concatenation**: [batch_size, 2 * embedding_dim + 1]\n",
    "\n",
    "*     **Sum**: [batch_size, embedding_dim + 1]\n",
    "\n",
    "*     **Mean**: [batch_size, embedding_dim + 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd74ULgpnJrc"
   },
   "source": [
    "# Performance evaluation\n",
    "\n",
    "Due to our simplifications, obtained results are not directly compatible with a traditional fact checking method that considers the evidence set as a whole.\n",
    "\n",
    "Thus, we need to consider two types of evaluations.\n",
    "\n",
    "**Multi-input classification evaluation**\n",
    "\n",
    "This type of evaluation is the easiest and concerns computing evaluation metrics, such as accuracy, f1-score, recall and precision, of our pre-processed dataset.\n",
    "\n",
    "In other words, we assess the performance of chosen classifiers.\n",
    "\n",
    "**Claim verification evaluation**\n",
    "\n",
    "However, if we want to give an answer concerning the claim itself, we need to consider the whole evidence set. \n",
    "\n",
    "Intuitively, for a given claim, we consider all its corresponding (claim, evidence) pairs and their corresponding classification outputs. \n",
    "\n",
    "At this point, all we need to do is to compute the final predicted claim label via majority voting.\n",
    "\n",
    "Example:\n",
    "\n",
    "    Claim: c1\n",
    "    Evidence set: e1, e2, e3\n",
    "    True label: S\n",
    "\n",
    "    Pair outputs:\n",
    "    (c1, e1) -> S (supports)\n",
    "    (c1, e2) -> S (supports)\n",
    "    (c1, e3) -> R (refutes)\n",
    "\n",
    "    Majority voting:\n",
    "    S -> 2 votes\n",
    "    R -> 1 vote\n",
    "\n",
    "    Final label:\n",
    "    c1 -> S\n",
    "\n",
    "Lastly, we have to compute classification metrics just like before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4LJ2yPxsUOV"
   },
   "source": [
    "# Tips and Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf80UVRNrXve"
   },
   "source": [
    "## Extensions are welcome!\n",
    "\n",
    "Is this task too easy for you? Are you curious to try out things you have seen during lectures (e.g. attention)? Feel free to try everything you want!\n",
    "\n",
    "Don't forget to try neural baselines first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COXeCXdYsBEf"
   },
   "source": [
    "## Comments and documentation\n",
    "\n",
    "Remember to properly comment your code (it is not necessary to comment each single line) and don't forget to describe your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejv6SDE8xc4_"
   },
   "source": [
    "## Organization\n",
    "\n",
    "We suggest you to divide your work into sections. This allows you to build clean and modular code, as well as easy to read and to debug.\n",
    "\n",
    "A possible schema:\n",
    "\n",
    "*   Dataset pre-processing\n",
    "*   Dataset conversion\n",
    "*   Model definition\n",
    "*   Training\n",
    "*   Evaluation\n",
    "*   Comments/Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DR70uh7pabo"
   },
   "source": [
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Andrea Galassi -> a.galassi@unibo.it\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it\n",
    "\n",
    "Don't forget that your feedback is very important! Your suggestions help us improving course material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc0gNWU2pgKQ"
   },
   "source": [
    "# FAQ\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Can I do something text pre-processing?**\n",
    "\n",
    "**A:** You have to! If you check text data, the majority of sentences need some cleaning.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: I'm struggling with the implementation. Can you help me?**\n",
    "\n",
    "**A:** Yes sure! Write us an email about your issue. If you are looking for a particular type of operation, you can easily check the documentation of the deep learning framework you are using (google is your friend).\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Can I try other encoding strategies or neural architectures?**\n",
    "\n",
    "**A:** Absolutely! Remember to try out recommended neural baselines first and only then proceed with your extensions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_stjmjcSx7cO"
   },
   "source": [
    "## Dataset pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnEY9yI3y0aU"
   },
   "source": [
    "### Dataset loading and inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saWDqtJyx-90",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# wider pandas columns\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "train_df = pd.read_csv(\"dataset/train_pairs.csv\")\n",
    "val_df   = pd.read_csv(\"dataset/val_pairs.csv\")\n",
    "test_df  = pd.read_csv(\"dataset/test_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxsqxB6zyTXq",
    "outputId": "5a052bad-5dcf-42aa-e6fe-4d926f045acf",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Chris Hemsworth appeared in A Perfect Getaway.</td>\n",
       "      <td>2\\tHemsworth has also appeared in the science fiction action film Star Trek -LRB- 2009 -RRB- , the thriller adventure A Perfect Getaway -LRB- 2009 -RRB- , the horror comedy The Cabin in the Woods -LRB- 2012 -RRB- , the dark-fantasy action film Snow White and the Huntsman -LRB- 2012 -RRB- , the war film Red Dawn -LRB- 2012 -RRB- , and the biographical sports drama film Rush -LRB- 2013 -RRB- .\\tStar Trek\\tStar Trek (film)\\tA Perfect Getaway\\tA Perfect Getaway\\tThe Cabin in the Woods\\tThe Cabin in the Woods\\tSnow White and the Huntsman\\tSnow White and the Huntsman\\tRed Dawn\\tRed Dawn (2012 film)\\tRush\\tRush (2013 film)</td>\n",
       "      <td>3</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Roald Dahl is a writer.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈdɑːl -RSB- , -LSB- ˈɾuːɑl dɑl -RSB- ; 13 September 1916 -- 23 November 1990 -RRB- was a British novelist , short story writer , poet , screenwriter , and fighter pilot .\\tfighter pilot\\tfighter pilot</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Roald Dahl is a governor.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈdɑːl -RSB- , -LSB- ˈɾuːɑl dɑl -RSB- ; 13 September 1916 -- 23 November 1990 -RRB- was a British novelist , short story writer , poet , screenwriter , and fighter pilot .\\tfighter pilot\\tfighter pilot</td>\n",
       "      <td>8</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ireland has relatively low-lying mountains.</td>\n",
       "      <td>10\\tThe island 's geography comprises relatively low-lying mountains surrounding a central plain , with several navigable rivers extending inland .\\tisland\\tisland\\tgeography\\tgeography\\tseveral navigable rivers\\tRivers of Ireland</td>\n",
       "      <td>9</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ireland does not have relatively low-lying mountains.</td>\n",
       "      <td>10\\tThe island 's geography comprises relatively low-lying mountains surrounding a central plain , with several navigable rivers extending inland .\\tisland\\tisland\\tgeography\\tgeography\\tseveral navigable rivers\\tRivers of Ireland</td>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121734</th>\n",
       "      <td>121734</td>\n",
       "      <td>Anderson Silva is a former UFC heavyweight Champion.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229439</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121735</th>\n",
       "      <td>121735</td>\n",
       "      <td>April was the month Anderson Silva was born.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229440</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121736</th>\n",
       "      <td>121736</td>\n",
       "      <td>Anderson Silva is an American Brazilian mixed martial artist.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229443</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121737</th>\n",
       "      <td>121737</td>\n",
       "      <td>Anderson Silva is incapable of being a Brazilian mixed martial artist.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229444</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121738</th>\n",
       "      <td>121738</td>\n",
       "      <td>Anderson Silva was born on the month of April 14th, 1975.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229445</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121739 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  ...     Label\n",
       "0                0  ...  SUPPORTS\n",
       "1                1  ...  SUPPORTS\n",
       "2                2  ...   REFUTES\n",
       "3                3  ...  SUPPORTS\n",
       "4                4  ...   REFUTES\n",
       "...            ...  ...       ...\n",
       "121734      121734  ...   REFUTES\n",
       "121735      121735  ...  SUPPORTS\n",
       "121736      121736  ...   REFUTES\n",
       "121737      121737  ...   REFUTES\n",
       "121738      121738  ...  SUPPORTS\n",
       "\n",
       "[121739 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect dataset\n",
    "train_df.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf0sClvX4Wi0"
   },
   "source": [
    "### Preprocessing\n",
    "Now we are going to apply some preprocessing to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kHLyxhH4Vlu",
    "outputId": "fdfe98fd-83b3-4615-9350-e804113ba1ed",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\tHemsworth has also appeared in the science fiction action film Star Trek -LRB- 2009 -RRB- , the thriller adventure A Perfect Getaway -LRB- 2009 -RRB- , the horror comedy The Cabin in the Woods -LRB- 2012 -RRB- , the dark-fantasy action film Snow White and the Huntsman -LRB- 2012 -RRB- , the war film Red Dawn -LRB- 2012 -RRB- , and the biographical sports drama film Rush -LRB- 2013 -RRB- .\tStar Trek\tStar Trek (film)\tA Perfect Getaway\tA Perfect Getaway\tThe Cabin in the Woods\tThe Cabin in the Woods\tSnow White and the Huntsman\tSnow White and the Huntsman\tRed Dawn\tRed Dawn (2012 film)\tRush\tRush (2013 film)\n",
      "25\tShe has appeared in Time 100 most influential people in the world -LRB- 2010 and 2015 -RRB- , Forbes top-earning women in music -LRB- 2011 -- 2015 -RRB- , Forbes 100 most powerful women -LRB- 2015 -RRB- , and Forbes Celebrity 100 -LRB- 2016 -RRB- .\tTime\tTime (magazine)\t100 most influential people in the world\tTime 100\tForbes\tForbes\t100 most powerful women\tThe World's 100 Most Powerful Women\n"
     ]
    }
   ],
   "source": [
    "# examine evidences\n",
    "print(train_df[\"Evidence\"][0])\n",
    "print(train_df[\"Evidence\"][993])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDEwLHAv7ZaE",
    "outputId": "47bd5a0e-94ec-4a08-8504-afdeeaa3bb89",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
    "\n",
    "def remove_leading_tabs(text) :\n",
    "  # remove leading tabs\n",
    "  pattern = r'[0-9]+?\\t'\n",
    "  return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_pronunciations(text) :\n",
    "  # remove pronunciations\n",
    "  pattern = r'-LSB-.*?-RSB-(\\s;)*?'\n",
    "  return re.sub(pattern, '', text)\n",
    "\n",
    "def convert_round_brackets(text) :\n",
    "  # convert -LRB- -RRB- to ( or )\n",
    "  pattern = r'-LRB-'\n",
    "  text = re.sub(pattern, '(', text)\n",
    "  pattern = r'-RRB-'\n",
    "  return re.sub(pattern, ')', text)\n",
    "\n",
    "def fix_double_dashes(text) :\n",
    "  # fix: double dashes (--)\n",
    "  pattern = r'\\-\\-'\n",
    "  return re.sub(pattern, '-', text)\n",
    "\n",
    "def remove_trailing_words(text) :\n",
    "  # remove trailing words (hyperlinks)\n",
    "  pattern = r'.\\t.*?$'\n",
    "  return re.sub(pattern, '.', text)\n",
    "\n",
    "def split_genitive(text) :\n",
    "  # make sure all possesive 's are split from other words\n",
    "  pattern = r\"(\\s.+?)'s\"\n",
    "  return re.sub(pattern, r\"\\1's\", text)\n",
    "\n",
    "def split_periods(text) :\n",
    "  pattern = r'(\\s.+?)\\.'\n",
    "  return re.sub(pattern, r'\\1 .', text)\n",
    "\n",
    "def fix_days(text) :\n",
    "  # fix: 31st -> 31 st\n",
    "  pattern = r'([0-9]{1,2})(st|nd|rd|th)'\n",
    "  return re.sub(pattern, r'\\1', text)\n",
    "\n",
    "def separate_years(text) :\n",
    "  # fix: separate years from other words\n",
    "  pattern = r'(\\s.+?)([0-9]{4})'\n",
    "  return re.sub(pattern, r'\\1 \\2', text) \n",
    "\n",
    "def fix_comma_thousands(text) :\n",
    "  # fix: comma thousands notations \n",
    "  pattern = r'([0-9]{1,3}),([0-9]{1,3})'\n",
    "  text = re.sub(pattern, r'\\1\\2', text)\n",
    "  pattern = r'([0-9]{1,3}),'\n",
    "  return re.sub(pattern, r'\\1', text)\n",
    "\n",
    "def fix_weird_dash(text) :\n",
    "  # fix: replace weird dash with normal one\n",
    "  pattern = r'–'\n",
    "  return re.sub(pattern, '-', text)\n",
    "\n",
    "def fix_years_ranges(text) :\n",
    "  # fix years ranges\n",
    "  pattern = r'([0-9]{4})\\-([0-9]{4})'\n",
    "  text = re.sub(pattern, r'\\1 - \\2', text)\n",
    "  pattern = r'([0-9]{2})([0-9]{2})\\-([0-9]{2})'\n",
    "  text = re.sub(pattern, r'\\1\\2 - \\1\\3', text)\n",
    "  pattern = r'\\'([0-9]{2})-\\'([0-9]{2})'\n",
    "  return re.sub(pattern, r'19\\1 - 19\\2', text)\n",
    "\n",
    "def fix_number_ranges(text) :\n",
    "  # fix: numbers ranges\n",
    "  pattern = r'([0-9]+?[,\\.][0-9]+?)+?-([0-9]+?[,\\.][0-9]+?)+'\n",
    "  return re.sub(pattern, r'\\1 - \\2', text)\n",
    "\n",
    "def fix_double_tick(text) :\n",
    "  # fix: double tick\n",
    "  pattern = r'\\`\\`'\n",
    "  return re.sub(pattern, '\"', text)\n",
    "\n",
    "def fix_date_merged(text) :\n",
    "  # fix: year/day merged with other word\n",
    "  pattern = r'([0-9]{1,4})([a-zA-Z]+?)'\n",
    "  return re.sub(pattern, r'\\1 \\2', text)\n",
    "\n",
    "def fix_double_ending_periods(text) :\n",
    "  # fix: double ending periods in claims\n",
    "  pattern = r'([a-zA-Z]{1,2}\\.)\\.$'               # except abbreviations (e.g jr. or c. k.)\n",
    "  text = re.sub(pattern, '\\1 .', text)\n",
    "  pattern = r'\\.\\.$'\n",
    "  return re.sub(pattern, '.', text)\n",
    "\n",
    "def fix_slashes_words(text) :\n",
    "  # fix: slashes separated from second word/number  e.g. \"2006/ 2007\"\n",
    "  pattern = r'(\\s.+?)\\/\\s(.+?\\s)'\n",
    "  text =  re.sub(pattern, r'\\1 \\/ \\2', text)    \n",
    "  # fix: separate strings between slashes\n",
    "  pattern = r'(.+?)\\/(.+?)'\n",
    "  return re.sub(pattern, r'\\1 \\/ \\2', text)\n",
    "\n",
    "def fix_string_ending_dash(text) :\n",
    "  # fix: fix strings ending with dash\n",
    "  pattern = r'(.+?)\\-\\s'\n",
    "  return re.sub(pattern, r'\\1 - ', text)\n",
    "\n",
    "def separate_non_words(text) :\n",
    "  # fix: separate words like non-something\n",
    "  pattern = r'([a-zA-Z]+?)\\-([a-zA-Z]+?)'\n",
    "  return re.sub(pattern, r'\\1 - \\2', text)\n",
    "  \n",
    "def fix_remove_round_brackets(text) :\n",
    "  # remove between round brackets\n",
    "  pattern = r'\\([^\\(\\)]+?\\)'\n",
    "  return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_double_spaces(text) :\n",
    "  # remove double spaces\n",
    "  pattern = r'(\\s)\\s+?'\n",
    "  return re.sub(pattern, r'\\1', text)\n",
    "\n",
    "#test\n",
    "def replace_special_characters(text):\n",
    "  REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "  return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "def good_symbols(text) :\n",
    "  GOOD_SYMBOLS_RE = re.compile('[^0-9a-zA-Z\\s\\+_]')\n",
    "  return GOOD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "def lower(text) :\n",
    "  return text.lower()\n",
    "\n",
    "PREPROCESSING_PIPELINE = [\n",
    "                          remove_leading_tabs,\n",
    "                          remove_trailing_words,\n",
    "                          #remove_stopwords,\n",
    "                          convert_round_brackets,\n",
    "                          #fix_remove_round_brackets,\n",
    "                          remove_pronunciations,\n",
    "                          fix_double_dashes,\n",
    "                          split_genitive,\n",
    "                          split_periods,\n",
    "                          fix_days,\n",
    "                          separate_years,\n",
    "                          fix_comma_thousands,\n",
    "                          fix_weird_dash,\n",
    "                          fix_years_ranges,\n",
    "                          fix_number_ranges,\n",
    "                          fix_double_tick,\n",
    "                          fix_date_merged,\n",
    "                          fix_double_ending_periods,\n",
    "                          fix_slashes_words,\n",
    "                          fix_string_ending_dash,\n",
    "                          separate_non_words,\n",
    "                          remove_double_spaces,\n",
    "                          #replace_special_characters,\n",
    "                          #good_symbols,\n",
    "                          lower\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess_text(text, filter_methods=None):\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxzbEgrvEmVj",
    "outputId": "085d312c-bf51-48e0-b04c-83c539d5b2b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n",
      "Training data done.\n",
      "Validation data done.\n",
      "Testing data done.\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing dataset...\")\n",
    "\n",
    "train_df[\"Claim\"] = train_df[\"Claim\"].apply(preprocess_text)\n",
    "train_df[\"Evidence\"] = train_df[\"Evidence\"].apply(preprocess_text)\n",
    "print(\"Training data done.\")\n",
    "val_df[\"Claim\"] = val_df[\"Claim\"].apply(preprocess_text)\n",
    "val_df[\"Evidence\"] = val_df[\"Evidence\"].apply(preprocess_text)\n",
    "print(\"Validation data done.\")\n",
    "test_df[\"Claim\"] = test_df[\"Claim\"].apply(preprocess_text)\n",
    "test_df[\"Evidence\"] = test_df[\"Evidence\"].apply(preprocess_text)\n",
    "print(\"Testing data done.\")\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOAg0oo-9C4v",
    "outputId": "47c5a5b3-b963-4a10-8211-599d63fab11c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chris hemsworth appeared in a perfect getaway .\n",
      "hemsworth has also appeared in the science fiction action film star trek ( 2009 ) , the thriller adventure a perfect getaway ( 2009 ) , the horror comedy the cabin in the woods ( 2012 ) , the dark - fantasy action film snow white and the huntsman ( 2012 ) , the war film red dawn ( 2012 ) , and the biographical sports drama film rush ( 2013 ) .\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"Claim\"][0])\n",
    "print(train_df[\"Evidence\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "0wm-HZRtkJ4W",
    "outputId": "7f3231ce-7664-4103-fa9b-8f98d02d3c00",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chris hemsworth appeared in a perfect getaway .</td>\n",
       "      <td>hemsworth has also appeared in the science fiction action film star trek ( 2009 ) , the thriller adventure a perfect getaway ( 2009 ) , the horror comedy the cabin in the woods ( 2012 ) , the dark - fantasy action film snow white and the huntsman ( 2012 ) , the war film red dawn ( 2012 ) , and the biographical sports drama film rush ( 2013 ) .</td>\n",
       "      <td>3</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>roald dahl is a writer .</td>\n",
       "      <td>roald dahl ( , ; 13 september 1916 - 23 november 1990 ) was a british novelist , short story writer , poet , screenwriter , and fighter pilot .</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>roald dahl is a governor .</td>\n",
       "      <td>roald dahl ( , ; 13 september 1916 - 23 november 1990 ) was a british novelist , short story writer , poet , screenwriter , and fighter pilot .</td>\n",
       "      <td>8</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ireland has relatively low - lying mountains .</td>\n",
       "      <td>the island 's geography comprises relatively low - lying mountains surrounding a central plain , with several navigable rivers extending inland .</td>\n",
       "      <td>9</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ireland does not have relatively low - lying mountains .</td>\n",
       "      <td>the island 's geography comprises relatively low - lying mountains surrounding a central plain , with several navigable rivers extending inland .</td>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121735</th>\n",
       "      <td>121735</td>\n",
       "      <td>april was the month anderson silva was born .</td>\n",
       "      <td>anderson da silva ( ; born april 14 , 1975 ) is a brazilian mixed martial artist and former ufc middleweight champion .</td>\n",
       "      <td>229440</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121736</th>\n",
       "      <td>121736</td>\n",
       "      <td>anderson silva is an american brazilian mixed martial artist .</td>\n",
       "      <td>anderson da silva ( ; born april 14 , 1975 ) is a brazilian mixed martial artist and former ufc middleweight champion .</td>\n",
       "      <td>229443</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121737</th>\n",
       "      <td>121737</td>\n",
       "      <td>anderson silva is incapable of being a brazilian mixed martial artist .</td>\n",
       "      <td>anderson da silva ( ; born april 14 , 1975 ) is a brazilian mixed martial artist and former ufc middleweight champion .</td>\n",
       "      <td>229444</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121738</th>\n",
       "      <td>121738</td>\n",
       "      <td>anderson silva was born on the month of april 14 1975 .</td>\n",
       "      <td>anderson da silva ( ; born april 14 , 1975 ) is a brazilian mixed martial artist and former ufc middleweight champion .</td>\n",
       "      <td>229445</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121739</th>\n",
       "      <td>121739</td>\n",
       "      <td>anderson silva was born on the day of the 15 .</td>\n",
       "      <td>anderson da silva ( ; born april 14 , 1975 ) is a brazilian mixed martial artist and former ufc middleweight champion .</td>\n",
       "      <td>229448</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121740 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  ...     Label\n",
       "0                0  ...  SUPPORTS\n",
       "1                1  ...  SUPPORTS\n",
       "2                2  ...   REFUTES\n",
       "3                3  ...  SUPPORTS\n",
       "4                4  ...   REFUTES\n",
       "...            ...  ...       ...\n",
       "121735      121735  ...  SUPPORTS\n",
       "121736      121736  ...   REFUTES\n",
       "121737      121737  ...   REFUTES\n",
       "121738      121738  ...  SUPPORTS\n",
       "121739      121739  ...   REFUTES\n",
       "\n",
       "[121740 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "XdxgVMjVaWo9",
    "outputId": "7cf0463c-5341-4036-9d84-d053a463c3f2",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Claim, Evidence, ID, Label]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['Claim'].str.contains('k\\.\\.') == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8kXXkECfun"
   },
   "source": [
    "## Dataset conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTOKlA6yNvFh",
    "outputId": "ddea9daf-eb23-40a0-fe33-4babe57523d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "embedding_dimension = 50\n",
    "glove = api.load(f\"glove-wiki-gigaword-{embedding_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gh0ZbOrs-MoK",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tokenizer based on penn treebank\n",
    "def tokenize_df(df) :\n",
    "  word_tokenizer = ToktokTokenizer()\n",
    "  tmp = pd.DataFrame(columns=[\"Claim\", \"Evidence\", \"Label\"])\n",
    "  for col in [\"Claim\", \"Evidence\"]:\n",
    "    tmp[col] = df[col].apply(word_tokenizer.tokenize)\n",
    "  tmp[\"Label\"] = df[\"Label\"]\n",
    "  return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8BJb7NqfVNk",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embedding_model, tokenizer) :\n",
    "  embedding_matrix = []\n",
    "  emebedding_dimension = embedding_model.vectors[0].shape[0]\n",
    "  for word in tokenizer.index_word.values() :\n",
    "    if word in embedding_model:\n",
    "      embedding_matrix.append(embedding_model[word])\n",
    "    else:\n",
    "      embedding_matrix.append(np.random.uniform(low=-1.0, high=1.0, size=(embedding_dimension,)))\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxCmr7Gdesue",
    "outputId": "e09fc672-3e5a-4ace-e733-864f12128e59",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[chris, hemsworth, appeared, in, a, perfect, getaway, .]</td>\n",
       "      <td>[hemsworth, has, also, appeared, in, the, science, fiction, action, film, star, trek, (, 2009, ), ,, the, thriller, adventure, a, perfect, getaway, (, 2009, ), ,, the, horror, comedy, the, cabin, in, the, woods, (, 2012, ), ,, the, dark, -, fantasy, action, film, snow, white, and, the, huntsman, (, 2012, ), ,, the, war, film, red, dawn, (, 2012, ), ,, and, the, biographical, sports, drama, film, rush, (, 2013, ), .]</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[roald, dahl, is, a, writer, .]</td>\n",
       "      <td>[roald, dahl, (, ,, ;, 13, september, 1916, -, 23, november, 1990, ), was, a, british, novelist, ,, short, story, writer, ,, poet, ,, screenwriter, ,, and, fighter, pilot, .]</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[roald, dahl, is, a, governor, .]</td>\n",
       "      <td>[roald, dahl, (, ,, ;, 13, september, 1916, -, 23, november, 1990, ), was, a, british, novelist, ,, short, story, writer, ,, poet, ,, screenwriter, ,, and, fighter, pilot, .]</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ireland, has, relatively, low, -, lying, mountains, .]</td>\n",
       "      <td>[the, island, ', s, geography, comprises, relatively, low, -, lying, mountains, surrounding, a, central, plain, ,, with, several, navigable, rivers, extending, inland, .]</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ireland, does, not, have, relatively, low, -, lying, mountains, .]</td>\n",
       "      <td>[the, island, ', s, geography, comprises, relatively, low, -, lying, mountains, surrounding, a, central, plain, ,, with, several, navigable, rivers, extending, inland, .]</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Claim  ...     Label\n",
       "0             [chris, hemsworth, appeared, in, a, perfect, getaway, .]  ...  SUPPORTS\n",
       "1                                      [roald, dahl, is, a, writer, .]  ...  SUPPORTS\n",
       "2                                    [roald, dahl, is, a, governor, .]  ...   REFUTES\n",
       "3              [ireland, has, relatively, low, -, lying, mountains, .]  ...  SUPPORTS\n",
       "4  [ireland, does, not, have, relatively, low, -, lying, mountains, .]  ...   REFUTES\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_train_df = tokenize_df(train_df)\n",
    "tok_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RShM58yRk5xg",
    "outputId": "134b5deb-e63d-4abc-d0e8-57fabcdaa6ec",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 403506\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "def build_vocab_from_df(df) :\n",
    "  ret = []\n",
    "  for col in [\"Claim\", \"Evidence\"] :\n",
    "    for r in df[col] :\n",
    "      ret += r\n",
    "  ret = pd.unique(ret)    # much faster than np.unique\n",
    "  return ret\n",
    "\n",
    "vocab = list(glove.vocab.keys())               # glove vocab\n",
    "\n",
    "vocab_v1 = np.array(build_vocab_from_df(tok_train_df), dtype=str)   # add unique terms from train_df\n",
    "oov_v1 = vocab_v1[~np.in1d(vocab_v1, vocab)] # find OOV terms\n",
    "vocab = np.concatenate((vocab, oov_v1))   #update vocab\n",
    "\n",
    "vocab_v2 = np.array(build_vocab_from_df(tokenize_df(val_df)), dtype=str)   # add unique terms from val_df\n",
    "oov_v2 = vocab_v2[~np.in1d(vocab_v2, vocab)]\n",
    "vocab = np.concatenate((vocab, oov_v2))\n",
    "\n",
    "vocab_v3 = np.array(build_vocab_from_df(tokenize_df(test_df)), dtype=str)   # add unique terms from val_df\n",
    "oov_v3 = vocab_v3[~np.in1d(vocab_v3, vocab)]\n",
    "vocab = np.concatenate((vocab, oov_v3))\n",
    "\n",
    "print(f\"vocab len: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMkztzj5MxKZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build word -> int encoding \n",
    "word_to_idx = dict(zip(vocab, range(1, len(vocab)+1))) # start from 1 to reserve 0 to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0FpcuZziZ5T",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_to_idx=word_to_idx) :\n",
    "  return [word_to_idx[w] for w in sent]\n",
    "\n",
    "def encode_df(tok_df, word_to_idx) :\n",
    "  enc_df = pd.DataFrame(columns=[\"Claim\", \"Evidence\", \"Label\"])\n",
    "  for col in [\"Claim\", \"Evidence\"] :\n",
    "    enc_df[col] = tok_df[col].apply(lambda s: encode_sent(s, word_to_idx))\n",
    "  enc_df[\"Label\"] = tok_df[\"Label\"].apply(lambda x: 1 if x==\"SUPPORTS\" else 0)\n",
    "  return enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9N5pMhWz-b-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode dataset\n",
    "enc_train_df = encode_df(tok_train_df, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaA0NzWbl03y",
    "outputId": "b301e32d-785a-4783-c9a7-d961c56c2367",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2103, 107954, 790, 7, 8, 2616, 20647, 3]</td>\n",
       "      <td>[107954, 32, 53, 790, 7, 1, 1122, 3955, 609, 320, 754, 9780, 24, 704, 25, 2, 1, 8966, 6041, 8, 2616, 20647, 24, 704, 25, 2, 1, 5989, 2842, 1, 7741, 7, 1, 2508, 24, 940, 25, 2, 1, 2238, 12, 5848, 609, 320, 2643, 299, 6, 1, 34011, 24, 940, 25, 2, 1, 137, 320, 640, 4650, 24, 940, 25, 2, 6, 1, 18899, 885, 2693, 320, 3993, 24, 1280, 25, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[53403, 21758, 15, 8, 1542, 3]</td>\n",
       "      <td>[53403, 21758, 24, 2, 90, 677, 442, 6907, 12, 1022, 488, 1456, 25, 16, 8, 298, 8998, 2, 637, 524, 1542, 2, 4820, 2, 12604, 2, 6, 3511, 2499, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[53403, 21758, 15, 8, 1005, 3]</td>\n",
       "      <td>[53403, 21758, 24, 2, 90, 677, 442, 6907, 12, 1022, 488, 1456, 25, 16, 8, 298, 8998, 2, 637, 524, 1542, 2, 4820, 2, 12604, 2, 6, 3511, 2499, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1323, 32, 2224, 654, 12, 4740, 2755, 3]</td>\n",
       "      <td>[1, 584, 58, 1535, 4214, 7817, 2224, 654, 12, 4740, 2755, 2724, 8, 324, 5406, 2, 18, 202, 30455, 4058, 5787, 8331, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1323, 261, 37, 34, 2224, 654, 12, 4740, 2755, 3]</td>\n",
       "      <td>[1, 584, 58, 1535, 4214, 7817, 2224, 654, 12, 4740, 2755, 2724, 8, 324, 5406, 2, 18, 202, 30455, 4058, 5787, 8331, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Claim  ... Label\n",
       "0          [2103, 107954, 790, 7, 8, 2616, 20647, 3]  ...     1\n",
       "1                     [53403, 21758, 15, 8, 1542, 3]  ...     1\n",
       "2                     [53403, 21758, 15, 8, 1005, 3]  ...     0\n",
       "3           [1323, 32, 2224, 654, 12, 4740, 2755, 3]  ...     1\n",
       "4  [1323, 261, 37, 34, 2224, 654, 12, 4740, 2755, 3]  ...     0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSlPHZ_eeg7g",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocab=vocab, embedding_model=glove, embedding_dimension=50) :\n",
    "  matrix = [np.zeros((embedding_dimension))] # first element reserved to padding and set to all zeros\n",
    "  for w in vocab :\n",
    "    if w in embedding_model.vocab :\n",
    "      matrix.append(embedding_model[w])\n",
    "    else:\n",
    "      matrix.append(np.random.uniform(low=-1.0, high=1.0, size=embedding_dimension))\n",
    "  return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KoXDFWFggYm",
    "outputId": "1e10eee5-0487-4532-a6c2-8fca9b602ffd",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403507, 50)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(vocab, glove, 50)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTBHN_H79zQd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8EgkVgv0G5s",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_length = pad_sequences(enc_train_df['Evidence'], padding='post').shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiuWDcydpcch",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "enc_val_df = encode_df(tokenize_df(val_df), word_to_idx)\n",
    "enc_test_df = encode_df(tokenize_df(test_df), word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdeqmHXT0ogd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pad_train_claim = pad_sequences(enc_train_df['Claim'], maxlen=max_length, padding='post')\n",
    "pad_train_evidence = pad_sequences(enc_train_df['Evidence'], maxlen=max_length, padding='post')\n",
    "\n",
    "pad_val_claim = pad_sequences(enc_val_df['Claim'], maxlen=max_length, padding='post')\n",
    "pad_val_evidence = pad_sequences(enc_val_df['Evidence'], maxlen=max_length, padding='post')\n",
    "\n",
    "pad_test_claim = pad_sequences(enc_test_df['Claim'], maxlen=max_length, padding='post')\n",
    "pad_test_evidence = pad_sequences(enc_test_df['Evidence'], maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClqPYKKYm33K",
    "outputId": "83f1a72d-edc6-460a-d341-4ed79fd83b0c",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((121740, 144), (121740, 144))"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_train_claim.shape, pad_train_evidence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnFTWyj3CnCW"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgzuV5-OgxCQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Bidirectional, \\\n",
    "                                    LSTM, GRU, Lambda, GlobalMaxPooling1D, GlobalAveragePooling1D, \\\n",
    "                                    Concatenate, Add, Average, Dropout, Flatten, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBJQgMhuEmeR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_embedding_layer(embedding_matrix, name=None) :\n",
    "    embedding_layer = Embedding(\n",
    "        embedding_matrix.shape[0],    # vocab size \n",
    "        embedding_matrix.shape[1],    # embedding dimension\n",
    "        weights = [embedding_matrix],\n",
    "        mask_zero = True,\n",
    "        name = name,\n",
    "        trainable = False\n",
    "    )\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the four embeddeing stategies in one function which by default is `average` which means that the token sequences are encoded through a RNN which averages all the output states.\n",
    "The parameter `multi_input` can be set also to:\n",
    "- **last_state**: means that the last state is taken as the sentence embedding\n",
    "- **mlp**: means that the token sequences are encoded via a MLP layer.\n",
    "- **bag of vectors**: means that the sentence embedding are computed as the mean of its token embeddings\n",
    "- **max**: means that the token sequences are encoded through a RNN which takes the max from all the output states. This technique work a bit better between the others, this is why we decided to introduce it even though was not asked to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsgHKP72EG3i",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_sent_emb(units, multi_input='average', name=None) :\n",
    "  sent_emb = Sequential(name=name)\n",
    "  if multi_input == 'last_state' :\n",
    "    sent_emb.add(Bidirectional(LSTM(units, return_sequences=False)))\n",
    "  elif multi_input == 'average' :\n",
    "    sent_emb.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
    "    sent_emb.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "  elif multi_input == 'max' :\n",
    "    sent_emb.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
    "    sent_emb.add(Lambda(lambda x: tf.reduce_max(x, axis=1)))\n",
    "  elif multi_input == 'mlp':\n",
    "    sent_emb.add(Flatten())\n",
    "    sent_emb.add(Dense(144, activation='relu'))\n",
    "    sent_emb.add(Dense(50, activation='relu'))\n",
    "  elif multi_input == 'bag_of_vectors' :\n",
    "    sent_emb.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "\n",
    "  return sent_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `merge_mode` in the `build_model()` function handles the multi-inputs strategies. In particular:\n",
    "- **concat**: representes the concatenation between the two inputs\n",
    "- **sum**: representes the sum between the two inputs\n",
    "- **mean**: representes the mean between the two inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrjJ9z2gtD2o",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "from keras import backend as K\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "def build_model(embedding_matrix, max_length, multi_input='average', merge_mode='concat', cos_sim_feature=False) :\n",
    "\n",
    "  claim = Input((max_length,), name='claim')\n",
    "  evidence = Input((max_length,), name='evidence')\n",
    "\n",
    "  embedding_layer = build_embedding_layer(embedding_matrix, \"glove_embedding\")  \n",
    "    \n",
    "  embedding_c = embedding_layer(claim)\n",
    "  embedding_e = embedding_layer(evidence)\n",
    "\n",
    "  sent_emb_c = build_sent_emb(max_length, multi_input=multi_input, name=\"sent_emb_claim\") (embedding_c)\n",
    "  sent_emb_e = build_sent_emb(max_length, multi_input=multi_input, name=\"sent_emb_evidence\") (embedding_e)\n",
    "\n",
    "  if merge_mode == 'concat' :\n",
    "    output = Concatenate(name='refined_input')([sent_emb_c, sent_emb_e])    # option 1\n",
    "  elif merge_mode == 'sum' :\n",
    "    output = Add()([sent_emb_c, sent_emb_e])                                # option 2\n",
    "  elif merge_mode == 'mean' :\n",
    "    output = Average()([sent_emb_c, sent_emb_e])                            # option 3\n",
    "  \n",
    "  if cos_sim_feature :\n",
    "    distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([sent_emb_c, sent_emb_e])\n",
    "    output = Concatenate(name='cossim_refined_input')([output, distance])          # co sim\n",
    "  output = Dense(max_length/2, activation='relu')(output)\n",
    "  output = Dropout(0.5)(output)\n",
    "  output = Dense(max_length/(2**2), activation='relu')(output)\n",
    "  output = Dense(max_length/(2**3), activation='relu')(output)\n",
    "  output = Dense(1, activation='sigmoid')(output)\n",
    "  return Model([claim, evidence], output)\n",
    "\n",
    "model = build_model(embedding_matrix, max_length, multi_input='max', merge_mode='mean', cos_sim_feature=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAsqp1XhCsEf"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = tf.reduce_sum(tf.math.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = tf.reduce_sum(tf.math.round(tf.clip_by_value(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = tf.reduce_sum(tf.math.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = tf.reduce_sum(tf.math.round(tf.clip_by_value(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emdezuhn3vlc",
    "outputId": "3ca4d808-11fe-40aa-a4fd-efe82e425cee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1903/1903 [==============================] - 88s 36ms/step - loss: 0.5193 - accuracy: 0.7621 - val_loss: 0.6320 - val_accuracy: 0.6629\n",
      "Epoch 2/20\n",
      "1903/1903 [==============================] - 64s 34ms/step - loss: 0.3996 - accuracy: 0.8324 - val_loss: 0.5793 - val_accuracy: 0.6917\n",
      "Epoch 3/20\n",
      "1903/1903 [==============================] - 64s 33ms/step - loss: 0.3706 - accuracy: 0.8424 - val_loss: 0.5102 - val_accuracy: 0.7301\n",
      "Epoch 4/20\n",
      "1185/1903 [=================>............] - ETA: 23s - loss: 0.3446 - accuracy: 0.8524"
     ]
    }
   ],
   "source": [
    "model.fit(x=(pad_train_claim, pad_train_evidence), \n",
    "          y=enc_train_df['Label'],\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_data=((pad_val_claim, pad_val_evidence), enc_val_df['Label']),\n",
    "          callbacks = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzxK6rQRC0pC"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate((pad_test_claim, pad_test_evidence), enc_test_df[\"Label\"], verbose=0)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(\"F1-score: {}\".format(f1_score))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Majority voting metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def build_df_from_model(model, df, pad_claims, pad_evidences) :\n",
    "  predictions = model.predict((pad_claims, pad_evidences))\n",
    "  pred_df = pd.DataFrame()\n",
    "  pred_df[\"Claim\"] = df[\"Claim\"]\n",
    "  pred_df[\"Label\"] = np.rint(predictions)\n",
    "  pred_df[\"Label\"] = pred_df[\"Label\"].apply(lambda x: \"SUPPORTS\" if x == 1.0 else \"REFUTES\")\n",
    "  return pred_df\n",
    "\n",
    "def get_majority_voting(df) :\n",
    "  unique_claims = pd.DataFrame(columns=[\"Claim\", \"Result\"])\n",
    "  unique_claims[\"Claim\"] = pd.unique(df[\"Claim\"])\n",
    "\n",
    "  for claim in unique_claims[\"Claim\"]:\n",
    "    labels = df[df[\"Claim\"] == claim]\n",
    "    supports = len(labels[labels[\"Label\"] == \"SUPPORTS\"])\n",
    "    refutes = len(labels[labels[\"Label\"] == \"REFUTES\"])\n",
    "    if supports > refutes :\n",
    "      unique_claims[\"Result\"][unique_claims[\"Claim\"] == claim] = 1\n",
    "    else :\n",
    "      unique_claims[\"Result\"][unique_claims[\"Claim\"] == claim] = 0\n",
    "\n",
    "  return unique_claims\n",
    "\n",
    "def compute_metrics_majority_voting(true, pred):\n",
    "  print(\"Accuracy:  \", accuracy_score(true, pred))\n",
    "  print(\"F1 score:  \", f1_score(true, pred))\n",
    "  print(\"Precision: \", precision_score(true, pred))\n",
    "  print(\"Recall:    \", recall_score(true, pred))\n",
    "\n",
    "trues = get_majority_voting(test_df)[\"Result\"].to_numpy(int)\n",
    "preds = build_df_from_model(model, test_df, pad_test_claim, pad_test_evidence)\n",
    "preds = get_majority_voting(preds)[\"Result\"].to_numpy(int)\n",
    "\n",
    "compute_metrics_majority_voting(trues, preds)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Uf80UVRNrXve",
    "COXeCXdYsBEf",
    "Ejv6SDE8xc4_",
    "TnEY9yI3y0aU",
    "VY8kXXkECfun",
    "dzxK6rQRC0pC"
   ],
   "name": "Assignment_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
