{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BspxZcRjW0NG",
    "outputId": "b8280819-b0c0-4e09-d8c4-8f76aca4bde8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading FEVER data splits...\n",
      "Download completed!\n",
      "Extracting dataset...\n",
      "Extraction completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_path):\n",
    "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
    "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
    "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(toy_data_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:\n",
    "            response = current_session.get(toy_url,\n",
    "                                   params={'id': toy_data_url_id},\n",
    "                                   stream=True)\n",
    "        save_response_content(response, toy_data_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
    "            loaded_zip.extractall(data_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "download_data('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_stjmjcSx7cO"
   },
   "source": [
    "## Dataset pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnEY9yI3y0aU"
   },
   "source": [
    "### Dataset loading and inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "saWDqtJyx-90",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# wider pandas columns\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "train_df = pd.read_csv(\"dataset/train_pairs.csv\")\n",
    "val_df   = pd.read_csv(\"dataset/val_pairs.csv\")\n",
    "test_df  = pd.read_csv(\"dataset/test_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IxsqxB6zyTXq",
    "outputId": "f9aac0ed-ef83-47f7-8d96-e8ba01f75215",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Chris Hemsworth appeared in A Perfect Getaway.</td>\n",
       "      <td>2\\tHemsworth has also appeared in the science fiction action film Star Trek -LRB- 2009 -RRB- , the thriller adventure A Perfect Getaway -LRB- 2009 -RRB- , the horror comedy The Cabin in the Woods -LRB- 2012 -RRB- , the dark-fantasy action film Snow White and the Huntsman -LRB- 2012 -RRB- , the war film Red Dawn -LRB- 2012 -RRB- , and the biographical sports drama film Rush -LRB- 2013 -RRB- .\\tStar Trek\\tStar Trek (film)\\tA Perfect Getaway\\tA Perfect Getaway\\tThe Cabin in the Woods\\tThe Cabin in the Woods\\tSnow White and the Huntsman\\tSnow White and the Huntsman\\tRed Dawn\\tRed Dawn (2012 film)\\tRush\\tRush (2013 film)</td>\n",
       "      <td>3</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Roald Dahl is a writer.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈdɑːl -RSB- , -LSB- ˈɾuːɑl dɑl -RSB- ; 13 September 1916 -- 23 November 1990 -RRB- was a British novelist , short story writer , poet , screenwriter , and fighter pilot .\\tfighter pilot\\tfighter pilot</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Roald Dahl is a governor.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈdɑːl -RSB- , -LSB- ˈɾuːɑl dɑl -RSB- ; 13 September 1916 -- 23 November 1990 -RRB- was a British novelist , short story writer , poet , screenwriter , and fighter pilot .\\tfighter pilot\\tfighter pilot</td>\n",
       "      <td>8</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ireland has relatively low-lying mountains.</td>\n",
       "      <td>10\\tThe island 's geography comprises relatively low-lying mountains surrounding a central plain , with several navigable rivers extending inland .\\tisland\\tisland\\tgeography\\tgeography\\tseveral navigable rivers\\tRivers of Ireland</td>\n",
       "      <td>9</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ireland does not have relatively low-lying mountains.</td>\n",
       "      <td>10\\tThe island 's geography comprises relatively low-lying mountains surrounding a central plain , with several navigable rivers extending inland .\\tisland\\tisland\\tgeography\\tgeography\\tseveral navigable rivers\\tRivers of Ireland</td>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121734</th>\n",
       "      <td>121734</td>\n",
       "      <td>Anderson Silva is a former UFC heavyweight Champion.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229439</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121735</th>\n",
       "      <td>121735</td>\n",
       "      <td>April was the month Anderson Silva was born.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229440</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121736</th>\n",
       "      <td>121736</td>\n",
       "      <td>Anderson Silva is an American Brazilian mixed martial artist.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229443</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121737</th>\n",
       "      <td>121737</td>\n",
       "      <td>Anderson Silva is incapable of being a Brazilian mixed martial artist.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229444</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121738</th>\n",
       "      <td>121738</td>\n",
       "      <td>Anderson Silva was born on the month of April 14th, 1975.</td>\n",
       "      <td>0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship</td>\n",
       "      <td>229445</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121739 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  \\\n",
       "0                0   \n",
       "1                1   \n",
       "2                2   \n",
       "3                3   \n",
       "4                4   \n",
       "...            ...   \n",
       "121734      121734   \n",
       "121735      121735   \n",
       "121736      121736   \n",
       "121737      121737   \n",
       "121738      121738   \n",
       "\n",
       "                                                                         Claim  \\\n",
       "0                               Chris Hemsworth appeared in A Perfect Getaway.   \n",
       "1                                                      Roald Dahl is a writer.   \n",
       "2                                                    Roald Dahl is a governor.   \n",
       "3                                  Ireland has relatively low-lying mountains.   \n",
       "4                        Ireland does not have relatively low-lying mountains.   \n",
       "...                                                                        ...   \n",
       "121734                    Anderson Silva is a former UFC heavyweight Champion.   \n",
       "121735                            April was the month Anderson Silva was born.   \n",
       "121736           Anderson Silva is an American Brazilian mixed martial artist.   \n",
       "121737  Anderson Silva is incapable of being a Brazilian mixed martial artist.   \n",
       "121738               Anderson Silva was born on the month of April 14th, 1975.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Evidence  \\\n",
       "0       2\\tHemsworth has also appeared in the science fiction action film Star Trek -LRB- 2009 -RRB- , the thriller adventure A Perfect Getaway -LRB- 2009 -RRB- , the horror comedy The Cabin in the Woods -LRB- 2012 -RRB- , the dark-fantasy action film Snow White and the Huntsman -LRB- 2012 -RRB- , the war film Red Dawn -LRB- 2012 -RRB- , and the biographical sports drama film Rush -LRB- 2013 -RRB- .\\tStar Trek\\tStar Trek (film)\\tA Perfect Getaway\\tA Perfect Getaway\\tThe Cabin in the Woods\\tThe Cabin in the Woods\\tSnow White and the Huntsman\\tSnow White and the Huntsman\\tRed Dawn\\tRed Dawn (2012 film)\\tRush\\tRush (2013 film)   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                 0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈdɑːl -RSB- , -LSB- ˈɾuːɑl dɑl -RSB- ; 13 September 1916 -- 23 November 1990 -RRB- was a British novelist , short story writer , poet , screenwriter , and fighter pilot .\\tfighter pilot\\tfighter pilot   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                 0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈdɑːl -RSB- , -LSB- ˈɾuːɑl dɑl -RSB- ; 13 September 1916 -- 23 November 1990 -RRB- was a British novelist , short story writer , poet , screenwriter , and fighter pilot .\\tfighter pilot\\tfighter pilot   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                10\\tThe island 's geography comprises relatively low-lying mountains surrounding a central plain , with several navigable rivers extending inland .\\tisland\\tisland\\tgeography\\tgeography\\tseveral navigable rivers\\tRivers of Ireland   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                10\\tThe island 's geography comprises relatively low-lying mountains surrounding a central plain , with several navigable rivers extending inland .\\tisland\\tisland\\tgeography\\tgeography\\tseveral navigable rivers\\tRivers of Ireland   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...   \n",
       "121734                                                                                                                                                                                                                                                                                                                                             0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship   \n",
       "121735                                                                                                                                                                                                                                                                                                                                             0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship   \n",
       "121736                                                                                                                                                                                                                                                                                                                                             0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship   \n",
       "121737                                                                                                                                                                                                                                                                                                                                             0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship   \n",
       "121738                                                                                                                                                                                                                                                                                                                                             0\\tAnderson da Silva -LRB- -LSB- ˈɐ̃deʁsõ ˈsiwvɐ -RSB- ; born April 14 , 1975 -RRB- is a Brazilian mixed martial artist and former UFC Middleweight Champion .\\tMiddleweight\\tMiddleweight (MMA)\\tmixed martial artist\\tmixed martial arts\\tUFC Middleweight Champion\\tUFC Middleweight Championship   \n",
       "\n",
       "            ID     Label  \n",
       "0            3  SUPPORTS  \n",
       "1            7  SUPPORTS  \n",
       "2            8   REFUTES  \n",
       "3            9  SUPPORTS  \n",
       "4           10   REFUTES  \n",
       "...        ...       ...  \n",
       "121734  229439   REFUTES  \n",
       "121735  229440  SUPPORTS  \n",
       "121736  229443   REFUTES  \n",
       "121737  229444   REFUTES  \n",
       "121738  229445  SUPPORTS  \n",
       "\n",
       "[121739 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect dataset\n",
    "train_df.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf0sClvX4Wi0"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kHLyxhH4Vlu",
    "outputId": "af9d8d47-df52-419b-9703-4dcd14e93465",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\tHemsworth has also appeared in the science fiction action film Star Trek -LRB- 2009 -RRB- , the thriller adventure A Perfect Getaway -LRB- 2009 -RRB- , the horror comedy The Cabin in the Woods -LRB- 2012 -RRB- , the dark-fantasy action film Snow White and the Huntsman -LRB- 2012 -RRB- , the war film Red Dawn -LRB- 2012 -RRB- , and the biographical sports drama film Rush -LRB- 2013 -RRB- .\tStar Trek\tStar Trek (film)\tA Perfect Getaway\tA Perfect Getaway\tThe Cabin in the Woods\tThe Cabin in the Woods\tSnow White and the Huntsman\tSnow White and the Huntsman\tRed Dawn\tRed Dawn (2012 film)\tRush\tRush (2013 film)\n",
      "25\tShe has appeared in Time 100 most influential people in the world -LRB- 2010 and 2015 -RRB- , Forbes top-earning women in music -LRB- 2011 -- 2015 -RRB- , Forbes 100 most powerful women -LRB- 2015 -RRB- , and Forbes Celebrity 100 -LRB- 2016 -RRB- .\tTime\tTime (magazine)\t100 most influential people in the world\tTime 100\tForbes\tForbes\t100 most powerful women\tThe World's 100 Most Powerful Women\n"
     ]
    }
   ],
   "source": [
    "# examine evidences\n",
    "print(train_df[\"Evidence\"][0])\n",
    "print(train_df[\"Evidence\"][993])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDEwLHAv7ZaE",
    "outputId": "9f14c948-3ca8-445e-d045-9b5405d96222",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
    "\n",
    "def remove_leading_tabs(text) :\n",
    "  # remove leading tabs\n",
    "  pattern = r'[0-9]+?\\t'\n",
    "  return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_pronunciations(text) :\n",
    "  # remove pronunciations\n",
    "  pattern = r'-LSB-.*?-RSB-(\\s;)*?'\n",
    "  return re.sub(pattern, '', text)\n",
    "\n",
    "def convert_round_brackets(text) :\n",
    "  # convert -LRB- -RRB- to ( or )\n",
    "  pattern = r'-LRB-'\n",
    "  text = re.sub(pattern, '(', text)\n",
    "  pattern = r'-RRB-'\n",
    "  return re.sub(pattern, ')', text)\n",
    "\n",
    "def fix_double_dashes(text) :\n",
    "  # fix: double dashes (--)\n",
    "  pattern = r'\\-\\-'\n",
    "  return re.sub(pattern, '-', text)\n",
    "\n",
    "def remove_trailing_words(text) :\n",
    "  # remove trailing words (hyperlinks)\n",
    "  pattern = r'.\\t.*?$'\n",
    "  return re.sub(pattern, '.', text)\n",
    "\n",
    "def split_genitive(text) :\n",
    "  # make sure all possesive 's are split from other words\n",
    "  pattern = r\"(\\s.+?)'s\"\n",
    "  return re.sub(pattern, r\"\\1's\", text)\n",
    "\n",
    "def split_periods(text) :\n",
    "  pattern = r'(\\s.+?)\\.'\n",
    "  return re.sub(pattern, r'\\1 .', text)\n",
    "\n",
    "def fix_days(text) :\n",
    "  # fix: 31st -> 31 st\n",
    "  pattern = r'([0-9]{1,2})(st|nd|rd|th)'\n",
    "  return re.sub(pattern, r'\\1', text)\n",
    "\n",
    "def separate_years(text) :\n",
    "  # fix: separate years from other words\n",
    "  pattern = r'(\\s.+?)([0-9]{4})'\n",
    "  return re.sub(pattern, r'\\1 \\2', text) \n",
    "\n",
    "def fix_comma_thousands(text) :\n",
    "  # fix: comma thousands notations \n",
    "  pattern = r'([0-9]{1,3}),([0-9]{1,3})'\n",
    "  text = re.sub(pattern, r'\\1\\2', text)\n",
    "  pattern = r'([0-9]{1,3}),'\n",
    "  return re.sub(pattern, r'\\1', text)\n",
    "\n",
    "def fix_weird_dash(text) :\n",
    "  # fix: replace weird dash with normal one\n",
    "  pattern = r'–'\n",
    "  return re.sub(pattern, '-', text)\n",
    "\n",
    "def fix_years_ranges(text) :\n",
    "  # fix years ranges\n",
    "  pattern = r'([0-9]{4})\\-([0-9]{4})'\n",
    "  text = re.sub(pattern, r'\\1 - \\2', text)\n",
    "  pattern = r'([0-9]{2})([0-9]{2})\\-([0-9]{2})'\n",
    "  text = re.sub(pattern, r'\\1\\2 - \\1\\3', text)\n",
    "  pattern = r'\\'([0-9]{2})-\\'([0-9]{2})'\n",
    "  return re.sub(pattern, r'19\\1 - 19\\2', text)\n",
    "\n",
    "def fix_number_ranges(text) :\n",
    "  # fix: numbers ranges\n",
    "  pattern = r'([0-9]+?[,\\.][0-9]+?)+?-([0-9]+?[,\\.][0-9]+?)+'\n",
    "  return re.sub(pattern, r'\\1 - \\2', text)\n",
    "\n",
    "def fix_double_tick(text) :\n",
    "  # fix: double tick\n",
    "  pattern = r'\\`\\`'\n",
    "  return re.sub(pattern, '\"', text)\n",
    "\n",
    "def fix_date_merged(text) :\n",
    "  # fix: year/day merged with other word\n",
    "  pattern = r'([0-9]{1,4})([a-zA-Z]+?)'\n",
    "  return re.sub(pattern, r'\\1 \\2', text)\n",
    "\n",
    "def fix_double_ending_periods(text) :\n",
    "  # fix: double ending periods in claims\n",
    "  pattern = r'([a-zA-Z]{1,2}\\.)\\.$'               # except abbreviations (e.g jr. or c. k.)\n",
    "  text = re.sub(pattern, '\\1 .', text)\n",
    "  pattern = r'\\.\\.$'\n",
    "  return re.sub(pattern, '.', text)\n",
    "\n",
    "def fix_slashes_words(text) :\n",
    "  # fix: slashes separated from second word/number  e.g. \"2006/ 2007\"\n",
    "  pattern = r'(\\s.+?)\\/\\s(.+?\\s)'\n",
    "  text =  re.sub(pattern, r'\\1 \\/ \\2', text)    \n",
    "  # fix: separate strings between slashes\n",
    "  pattern = r'(.+?)\\/(.+?)'\n",
    "  return re.sub(pattern, r'\\1 \\/ \\2', text)\n",
    "\n",
    "def fix_string_ending_dash(text) :\n",
    "  # fix: fix strings ending with dash\n",
    "  pattern = r'(.+?)\\-\\s'\n",
    "  return re.sub(pattern, r'\\1 - ', text)\n",
    "\n",
    "def separate_non_words(text) :\n",
    "  # fix: separate words like non-something\n",
    "  pattern = r'([a-zA-Z]+?)\\-([a-zA-Z]+?)'\n",
    "  return re.sub(pattern, r'\\1 - \\2', text)\n",
    "  \n",
    "def fix_remove_round_brackets(text) :\n",
    "  # remove between round brackets\n",
    "  pattern = r'\\([^\\(\\)]+?\\)'\n",
    "  return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_double_spaces(text) :\n",
    "  # remove double spaces\n",
    "  pattern = r'(\\s)\\s+?'\n",
    "  return re.sub(pattern, r'\\1', text)\n",
    "\n",
    "#test\n",
    "def replace_special_characters(text):\n",
    "  REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "  return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "def good_symbols(text) :\n",
    "  GOOD_SYMBOLS_RE = re.compile('[^0-9a-zA-Z\\s\\+_]')\n",
    "  return GOOD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "def lower(text) :\n",
    "  return text.lower()\n",
    "\n",
    "PREPROCESSING_PIPELINE = [\n",
    "                          remove_leading_tabs,\n",
    "                          remove_trailing_words,\n",
    "                          #remove_stopwords,\n",
    "                          convert_round_brackets,\n",
    "                          #fix_remove_round_brackets,\n",
    "                          remove_pronunciations,\n",
    "                          fix_double_dashes,\n",
    "                          split_genitive,\n",
    "                          split_periods,\n",
    "                          fix_days,\n",
    "                          separate_years,\n",
    "                          fix_comma_thousands,\n",
    "                          fix_weird_dash,\n",
    "                          fix_years_ranges,\n",
    "                          fix_number_ranges,\n",
    "                          fix_double_tick,\n",
    "                          fix_date_merged,\n",
    "                          fix_double_ending_periods,\n",
    "                          fix_slashes_words,\n",
    "                          fix_string_ending_dash,\n",
    "                          separate_non_words,\n",
    "                          remove_double_spaces,\n",
    "                          #replace_special_characters,\n",
    "                          #good_symbols,\n",
    "                          lower\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess_text(text, filter_methods=None):\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxzbEgrvEmVj",
    "outputId": "2def0e10-7856-4be6-c2b8-e495114fa898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n",
      "Training data done.\n",
      "Validation data done.\n",
      "Testing data done.\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing dataset...\")\n",
    "\n",
    "train_df[\"Claim\"] = train_df[\"Claim\"].apply(preprocess_text)\n",
    "train_df[\"Evidence\"] = train_df[\"Evidence\"].apply(preprocess_text)\n",
    "print(\"Training data done.\")\n",
    "val_df[\"Claim\"] = val_df[\"Claim\"].apply(preprocess_text)\n",
    "val_df[\"Evidence\"] = val_df[\"Evidence\"].apply(preprocess_text)\n",
    "print(\"Validation data done.\")\n",
    "test_df[\"Claim\"] = test_df[\"Claim\"].apply(preprocess_text)\n",
    "test_df[\"Evidence\"] = test_df[\"Evidence\"].apply(preprocess_text)\n",
    "print(\"Testing data done.\")\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "XdxgVMjVaWo9",
    "outputId": "b9a4120b-0ad8-46ca-dcc9-d28a9a79902b",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Claim, Evidence, ID, Label]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['Claim'].str.contains('k\\.\\.') == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8kXXkECfun"
   },
   "source": [
    "## Dataset conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTOKlA6yNvFh",
    "outputId": "23497a7f-f6db-424b-be9e-7c53d851bf94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "embedding_dimension = 50\n",
    "glove = api.load(f\"glove-wiki-gigaword-{embedding_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Gh0ZbOrs-MoK",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tokenizer based on penn treebank\n",
    "def tokenize_df(df) :\n",
    "  #word_tokenizer = TreebankWordTokenizer()\n",
    "  word_tokenizer = ToktokTokenizer()\n",
    "  tmp = pd.DataFrame(columns=[\"Claim\", \"Evidence\", \"Label\"])\n",
    "  for col in [\"Claim\", \"Evidence\"]:\n",
    "    #tmp[col] = df[col].str.lower()\n",
    "    tmp[col] = df[col].apply(word_tokenizer.tokenize)\n",
    "    #tmp[col] = np.array([word_tokenizer.tokenize(r) for r in tmp[col]])\n",
    "  tmp[\"Label\"] = df[\"Label\"]\n",
    "  return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J8BJb7NqfVNk",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embedding_model, tokenizer) :\n",
    "  embedding_matrix = []\n",
    "  emebedding_dimension = embedding_model.vectors[0].shape[0]\n",
    "  for word in tokenizer.index_word.values() :\n",
    "    if word in embedding_model:\n",
    "      embedding_matrix.append(embedding_model[word])\n",
    "    else:\n",
    "      embedding_matrix.append(np.random.uniform(low=-1.0, high=1.0, size=(embedding_dimension,)))\n",
    "  return embedding_matrix\n",
    "\n",
    "#np.array(get_embedding_matrix(glove, w_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "WxCmr7Gdesue",
    "outputId": "dc3aaf2a-1e11-438e-9ba0-5a57a8d2da40",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[chris, hemsworth, appeared, in, a, perfect, getaway, .]</td>\n",
       "      <td>[hemsworth, has, also, appeared, in, the, science, fiction, action, film, star, trek, (, 2009, ), ,, the, thriller, adventure, a, perfect, getaway, (, 2009, ), ,, the, horror, comedy, the, cabin, in, the, woods, (, 2012, ), ,, the, dark, -, fantasy, action, film, snow, white, and, the, huntsman, (, 2012, ), ,, the, war, film, red, dawn, (, 2012, ), ,, and, the, biographical, sports, drama, film, rush, (, 2013, ), .]</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[roald, dahl, is, a, writer, .]</td>\n",
       "      <td>[roald, dahl, (, ,, ;, 13, september, 1916, -, 23, november, 1990, ), was, a, british, novelist, ,, short, story, writer, ,, poet, ,, screenwriter, ,, and, fighter, pilot, .]</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[roald, dahl, is, a, governor, .]</td>\n",
       "      <td>[roald, dahl, (, ,, ;, 13, september, 1916, -, 23, november, 1990, ), was, a, british, novelist, ,, short, story, writer, ,, poet, ,, screenwriter, ,, and, fighter, pilot, .]</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ireland, has, relatively, low, -, lying, mountains, .]</td>\n",
       "      <td>[the, island, ', s, geography, comprises, relatively, low, -, lying, mountains, surrounding, a, central, plain, ,, with, several, navigable, rivers, extending, inland, .]</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ireland, does, not, have, relatively, low, -, lying, mountains, .]</td>\n",
       "      <td>[the, island, ', s, geography, comprises, relatively, low, -, lying, mountains, surrounding, a, central, plain, ,, with, several, navigable, rivers, extending, inland, .]</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Claim  \\\n",
       "0             [chris, hemsworth, appeared, in, a, perfect, getaway, .]   \n",
       "1                                      [roald, dahl, is, a, writer, .]   \n",
       "2                                    [roald, dahl, is, a, governor, .]   \n",
       "3              [ireland, has, relatively, low, -, lying, mountains, .]   \n",
       "4  [ireland, does, not, have, relatively, low, -, lying, mountains, .]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                              Evidence  \\\n",
       "0  [hemsworth, has, also, appeared, in, the, science, fiction, action, film, star, trek, (, 2009, ), ,, the, thriller, adventure, a, perfect, getaway, (, 2009, ), ,, the, horror, comedy, the, cabin, in, the, woods, (, 2012, ), ,, the, dark, -, fantasy, action, film, snow, white, and, the, huntsman, (, 2012, ), ,, the, war, film, red, dawn, (, 2012, ), ,, and, the, biographical, sports, drama, film, rush, (, 2013, ), .]   \n",
       "1                                                                                                                                                                                                                                                       [roald, dahl, (, ,, ;, 13, september, 1916, -, 23, november, 1990, ), was, a, british, novelist, ,, short, story, writer, ,, poet, ,, screenwriter, ,, and, fighter, pilot, .]   \n",
       "2                                                                                                                                                                                                                                                       [roald, dahl, (, ,, ;, 13, september, 1916, -, 23, november, 1990, ), was, a, british, novelist, ,, short, story, writer, ,, poet, ,, screenwriter, ,, and, fighter, pilot, .]   \n",
       "3                                                                                                                                                                                                                                                           [the, island, ', s, geography, comprises, relatively, low, -, lying, mountains, surrounding, a, central, plain, ,, with, several, navigable, rivers, extending, inland, .]   \n",
       "4                                                                                                                                                                                                                                                           [the, island, ', s, geography, comprises, relatively, low, -, lying, mountains, surrounding, a, central, plain, ,, with, several, navigable, rivers, extending, inland, .]   \n",
       "\n",
       "      Label  \n",
       "0  SUPPORTS  \n",
       "1  SUPPORTS  \n",
       "2   REFUTES  \n",
       "3  SUPPORTS  \n",
       "4   REFUTES  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_train_df = tokenize_df(train_df)\n",
    "tok_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RShM58yRk5xg",
    "outputId": "dd547fa3-2117-4266-ab76-5d1a9f2a619b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 403506\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "def build_vocab_from_df(df) :\n",
    "  ret = []\n",
    "  for col in [\"Claim\", \"Evidence\"] :\n",
    "    for r in df[col] :\n",
    "      ret += r\n",
    "  ret = pd.unique(ret)    # much faster than np.unique\n",
    "  return ret\n",
    "\n",
    "vocab = list(glove.vocab.keys())               # glove vocab\n",
    "\n",
    "vocab_v1 = np.array(build_vocab_from_df(tok_train_df), dtype=str)   # add unique terms from train_df\n",
    "oov_v1 = vocab_v1[~np.in1d(vocab_v1, vocab)] # find OOV terms\n",
    "vocab = np.concatenate((vocab, oov_v1))   #update vocab\n",
    "\n",
    "vocab_v2 = np.array(build_vocab_from_df(tokenize_df(val_df)), dtype=str)   # add unique terms from val_df\n",
    "oov_v2 = vocab_v2[~np.in1d(vocab_v2, vocab)]\n",
    "vocab = np.concatenate((vocab, oov_v2))\n",
    "\n",
    "vocab_v3 = np.array(build_vocab_from_df(tokenize_df(test_df)), dtype=str)   # add unique terms from val_df\n",
    "oov_v3 = vocab_v3[~np.in1d(vocab_v3, vocab)]\n",
    "vocab = np.concatenate((vocab, oov_v3))\n",
    "\n",
    "#vocab = list(dict.fromkeys(vocab))\n",
    "print(f\"vocab len: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aMkztzj5MxKZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build word -> int encoding \n",
    "word_to_idx = dict(zip(vocab, range(1, len(vocab)+1))) # start from 1 to reserve 0 to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "H0FpcuZziZ5T",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def encode_sent(sent, word_to_idx=word_to_idx) :\n",
    "  return [word_to_idx[w] for w in sent]\n",
    "\n",
    "def encode_df(tok_df, word_to_idx) :\n",
    "  enc_df = pd.DataFrame(columns=[\"Claim\", \"Evidence\", \"Label\"])\n",
    "  for col in [\"Claim\", \"Evidence\"] :\n",
    "    enc_df[col] = tok_df[col].apply(lambda s: encode_sent(s, word_to_idx))\n",
    "  enc_df[\"Label\"] = tok_df[\"Label\"].apply(lambda x: 1 if x==\"SUPPORTS\" else 0)\n",
    "  return enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "D9N5pMhWz-b-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode dataset\n",
    "enc_train_df = encode_df(tok_train_df, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "MaA0NzWbl03y",
    "outputId": "386b8c35-90c5-476a-b0df-8580164e5cfe",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2103, 107954, 790, 7, 8, 2616, 20647, 3]</td>\n",
       "      <td>[107954, 32, 53, 790, 7, 1, 1122, 3955, 609, 320, 754, 9780, 24, 704, 25, 2, 1, 8966, 6041, 8, 2616, 20647, 24, 704, 25, 2, 1, 5989, 2842, 1, 7741, 7, 1, 2508, 24, 940, 25, 2, 1, 2238, 12, 5848, 609, 320, 2643, 299, 6, 1, 34011, 24, 940, 25, 2, 1, 137, 320, 640, 4650, 24, 940, 25, 2, 6, 1, 18899, 885, 2693, 320, 3993, 24, 1280, 25, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[53403, 21758, 15, 8, 1542, 3]</td>\n",
       "      <td>[53403, 21758, 24, 2, 90, 677, 442, 6907, 12, 1022, 488, 1456, 25, 16, 8, 298, 8998, 2, 637, 524, 1542, 2, 4820, 2, 12604, 2, 6, 3511, 2499, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[53403, 21758, 15, 8, 1005, 3]</td>\n",
       "      <td>[53403, 21758, 24, 2, 90, 677, 442, 6907, 12, 1022, 488, 1456, 25, 16, 8, 298, 8998, 2, 637, 524, 1542, 2, 4820, 2, 12604, 2, 6, 3511, 2499, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1323, 32, 2224, 654, 12, 4740, 2755, 3]</td>\n",
       "      <td>[1, 584, 58, 1535, 4214, 7817, 2224, 654, 12, 4740, 2755, 2724, 8, 324, 5406, 2, 18, 202, 30455, 4058, 5787, 8331, 3]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1323, 261, 37, 34, 2224, 654, 12, 4740, 2755, 3]</td>\n",
       "      <td>[1, 584, 58, 1535, 4214, 7817, 2224, 654, 12, 4740, 2755, 2724, 8, 324, 5406, 2, 18, 202, 30455, 4058, 5787, 8331, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Claim  \\\n",
       "0          [2103, 107954, 790, 7, 8, 2616, 20647, 3]   \n",
       "1                     [53403, 21758, 15, 8, 1542, 3]   \n",
       "2                     [53403, 21758, 15, 8, 1005, 3]   \n",
       "3           [1323, 32, 2224, 654, 12, 4740, 2755, 3]   \n",
       "4  [1323, 261, 37, 34, 2224, 654, 12, 4740, 2755, 3]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                           Evidence  \\\n",
       "0  [107954, 32, 53, 790, 7, 1, 1122, 3955, 609, 320, 754, 9780, 24, 704, 25, 2, 1, 8966, 6041, 8, 2616, 20647, 24, 704, 25, 2, 1, 5989, 2842, 1, 7741, 7, 1, 2508, 24, 940, 25, 2, 1, 2238, 12, 5848, 609, 320, 2643, 299, 6, 1, 34011, 24, 940, 25, 2, 1, 137, 320, 640, 4650, 24, 940, 25, 2, 6, 1, 18899, 885, 2693, 320, 3993, 24, 1280, 25, 3]   \n",
       "1                                                                                                                                                                                                   [53403, 21758, 24, 2, 90, 677, 442, 6907, 12, 1022, 488, 1456, 25, 16, 8, 298, 8998, 2, 637, 524, 1542, 2, 4820, 2, 12604, 2, 6, 3511, 2499, 3]   \n",
       "2                                                                                                                                                                                                   [53403, 21758, 24, 2, 90, 677, 442, 6907, 12, 1022, 488, 1456, 25, 16, 8, 298, 8998, 2, 637, 524, 1542, 2, 4820, 2, 12604, 2, 6, 3511, 2499, 3]   \n",
       "3                                                                                                                                                                                                                             [1, 584, 58, 1535, 4214, 7817, 2224, 654, 12, 4740, 2755, 2724, 8, 324, 5406, 2, 18, 202, 30455, 4058, 5787, 8331, 3]   \n",
       "4                                                                                                                                                                                                                             [1, 584, 58, 1535, 4214, 7817, 2224, 654, 12, 4740, 2755, 2724, 8, 324, 5406, 2, 18, 202, 30455, 4058, 5787, 8331, 3]   \n",
       "\n",
       "   Label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dSlPHZ_eeg7g",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocab=vocab, embedding_model=glove, embedding_dimension=50) :\n",
    "  matrix = [np.zeros((embedding_dimension))] # first element reserved to padding and set to all zeros\n",
    "  for w in vocab :\n",
    "    if w in embedding_model.vocab :\n",
    "      matrix.append(embedding_model[w])\n",
    "    else:\n",
    "      matrix.append(np.random.uniform(low=-1.0, high=1.0, size=embedding_dimension))\n",
    "  return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KoXDFWFggYm",
    "outputId": "fa0f361d-1338-4b8b-e533-5c91002d99b3",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403507, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(vocab, glove, 50)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QTBHN_H79zQd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "a8EgkVgv0G5s",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_length = pad_sequences(enc_train_df['Evidence'], padding='post').shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "iiuWDcydpcch",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "enc_val_df = encode_df(tokenize_df(val_df), word_to_idx)\n",
    "enc_test_df = encode_df(tokenize_df(test_df), word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AdeqmHXT0ogd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pad_train_claim = pad_sequences(enc_train_df['Claim'], maxlen=max_length, padding='post')\n",
    "pad_train_evidence = pad_sequences(enc_train_df['Evidence'], maxlen=max_length, padding='post')\n",
    "\n",
    "pad_val_claim = pad_sequences(enc_val_df['Claim'], maxlen=max_length, padding='post')\n",
    "pad_val_evidence = pad_sequences(enc_val_df['Evidence'], maxlen=max_length, padding='post')\n",
    "\n",
    "pad_test_claim = pad_sequences(enc_test_df['Claim'], maxlen=max_length, padding='post')\n",
    "pad_test_evidence = pad_sequences(enc_test_df['Evidence'], maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClqPYKKYm33K",
    "outputId": "49f40f5c-f854-4fd0-949c-f864aec0b18c",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((121740, 144), (121740, 144))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_train_claim.shape, pad_train_evidence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnFTWyj3CnCW"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OgzuV5-OgxCQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Bidirectional, \\\n",
    "                                    LSTM, GRU, Lambda, GlobalMaxPooling1D, GlobalAveragePooling1D, \\\n",
    "                                    Concatenate, Add, Average, Dropout, Flatten, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.regularizers import L2\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# detect and init the TPU\n",
    "#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "#tf.config.experimental_connect_to_cluster(tpu)\n",
    "#tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "# instantiate a distribution strategy\n",
    "#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dBJQgMhuEmeR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_embedding_layer(embedding_matrix, name=None) :\n",
    "    embedding_layer = Embedding(\n",
    "        embedding_matrix.shape[0],    # vocab size \n",
    "        embedding_matrix.shape[1],    # embedding dimension\n",
    "        #embeddings_initializer=tf.initializers.Constant(embedding_matrix),\n",
    "        weights = [embedding_matrix],\n",
    "        mask_zero = True,\n",
    "        name = name,\n",
    "        trainable = False\n",
    "    )\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vsgHKP72EG3i",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_sent_emb(units, multi_input='average', name=None) :\n",
    "  sent_emb = Sequential(name=name)\n",
    "  if multi_input == 'last_state' :\n",
    "    sent_emb.add(Bidirectional(LSTM(units, return_sequences=False)))\n",
    "  elif multi_input == 'average' :\n",
    "    sent_emb.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
    "    sent_emb.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "  elif multi_input == 'max' :\n",
    "    sent_emb.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
    "    sent_emb.add(Lambda(lambda x: tf.reduce_max(x, axis=1)))\n",
    "  elif multi_input == 'mlp':\n",
    "    sent_emb.add(Flatten())\n",
    "    sent_emb.add(Dense(144, activation='relu'))\n",
    "    sent_emb.add(Dense(50, activation='relu'))\n",
    "  elif multi_input == 'bag_of_vectors' :\n",
    "    sent_emb.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "\n",
    "  return sent_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "hrjJ9z2gtD2o",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "from keras import backend as K\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "def build_model(embedding_matrix, max_length, multi_input='average', merge_mode='concat', cos_sim_feature=False) :\n",
    "\n",
    "  claim = Input((max_length,), name='claim')\n",
    "  evidence = Input((max_length,), name='evidence')\n",
    "\n",
    "  embedding_layer = build_embedding_layer(embedding_matrix, \"glove_embedding\")  \n",
    "    \n",
    "  embedding_c = embedding_layer(claim)\n",
    "  embedding_e = embedding_layer(evidence)\n",
    "\n",
    "  sent_emb_c = build_sent_emb(max_length, multi_input=multi_input, name=\"sent_emb_claim\") (embedding_c)\n",
    "  sent_emb_e = build_sent_emb(max_length, multi_input=multi_input, name=\"sent_emb_evidence\") (embedding_e)\n",
    "\n",
    "  #output = build_sent_emb(64,concat)\n",
    "\n",
    "  if merge_mode == 'concat' :\n",
    "    output = Concatenate(name='refined_input')([sent_emb_c, sent_emb_e])    # option 1\n",
    "  elif merge_mode == 'sum' :\n",
    "    output = Add()([sent_emb_c, sent_emb_e])                                # option 2\n",
    "  elif merge_mode == 'mean' :\n",
    "    output = Average()([sent_emb_c, sent_emb_e])                            # option 3\n",
    "  \n",
    "  if cos_sim_feature :\n",
    "    distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([sent_emb_c, sent_emb_e])\n",
    "    output = Concatenate(name='cossim_refined_input')([output, distance])          # co sim\n",
    "  output = Dense(max_length/2, activation='relu')(output)\n",
    "  output = Dropout(0.5)(output)\n",
    "  output = Dense(max_length/(2**2), activation='relu')(output)\n",
    "  #output = Dropout(0.5)(output)\n",
    "  output = Dense(max_length/(2**3), activation='relu')(output)\n",
    "  #output = Dropout(0.5)(output)\n",
    "  output = Dense(1, activation='softmax')(output)\n",
    "  return Model([claim, evidence], output)\n",
    "\n",
    "model = None\n",
    "#with tpu_strategy.scope():\n",
    "model = build_model(embedding_matrix, max_length, multi_input='max', merge_mode='mean', cos_sim_feature=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAsqp1XhCsEf"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IiDyD1yJyWFY",
    "outputId": "8f4bea31-7d70-4722-d4de-a4dce0e235b4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "claim (InputLayer)              [(None, 144)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "evidence (InputLayer)           [(None, 144)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "glove_embedding (Embedding)     (None, 144, 50)      20175350    claim[0][0]                      \n",
      "                                                                 evidence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sent_emb_claim (Sequential)     (None, 288)          224640      glove_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sent_emb_evidence (Sequential)  (None, 288)          224640      glove_embedding[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average (Average)               (None, 288)          0           sent_emb_claim[0][0]             \n",
      "                                                                 sent_emb_evidence[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           sent_emb_claim[0][0]             \n",
      "                                                                 sent_emb_evidence[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cossim_refined_input (Concatena (None, 289)          0           average[0][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 72)           20880       cossim_refined_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 72)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 36)           2628        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 18)           666         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            19          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,648,823\n",
      "Trainable params: 473,473\n",
      "Non-trainable params: 20,175,350\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emdezuhn3vlc",
    "outputId": "a65fa0cd-9b5f-4431-92a9-0dec5fd66da0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 121740 samples, validate on 7165 samples\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"EarlyStopping\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-90a4b5369e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0;31m#steps_per_epoch=5,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_val_claim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_val_evidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_val_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m           \u001b[0;31m#use_multiprocessing = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           )\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples_or_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Handle ProgBarLogger separately in this loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       mode=mode)\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;31m# TODO(omalleyt): Handle ProgBar as part of Callbacks once hooks are ready.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0mprogbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mconfigure_callbacks\u001b[0;34m(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBaseLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProgbarLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"EarlyStopping\") to list"
     ]
    }
   ],
   "source": [
    "model.fit(x=(pad_train_claim, pad_train_evidence), \n",
    "          y=enc_train_df['Label'],\n",
    "          batch_size=64,\n",
    "          epochs=1,\n",
    "          #steps_per_epoch=5,\n",
    "          validation_data=((pad_val_claim, pad_val_evidence), enc_val_df['Label']),\n",
    "          callbacks = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "          #use_multiprocessing = True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAXdkm3FEmVp",
    "outputId": "c9f09c76-8709-4fd7-a44a-3449d982fc89"
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate((pad_test_claim, pad_test_evidence), enc_test_df[\"Label\"], verbose=0)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(\"F1-score: {}\".format(f1_score))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jz0X3HCPZmwq",
    "outputId": "8ccee80b-a39f-4712-bd6f-95b9d31bb953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_true_labels(df_in):\n",
    "  acc = pd.DataFrame(columns=[\"Claim\", \"#Supports\", \"#Refutes\"])\n",
    "  for _, row in df_in.iterrows():\n",
    "    if row[\"Claim\"] in acc[\"Claim\"].tolist():\n",
    "      index = acc[\"Claim\"].tolist().index(row[\"Claim\"])\n",
    "      acc = acc.reset_index(drop=True)\n",
    "      if row[\"Label\"] == \"SUPPORTS\":\n",
    "        acc['#Supports'][index] = int(acc[\"#Supports\"][index]) + 1\n",
    "      else:\n",
    "        acc['#Refutes'][index] = int(acc[\"#Refutes\"][index]) + 1\n",
    "    else:\n",
    "      if row[\"Label\"] == \"SUPPORTS\":\n",
    "        tmp = pd.DataFrame([[row[\"Claim\"], 1, 0]], columns=[\"Claim\", \"#Supports\", \"#Refutes\"])\n",
    "      else:\n",
    "        tmp = pd.DataFrame([[row[\"Claim\"], 0, 1]], columns=[\"Claim\", \"#Supports\", \"#Refutes\"])\n",
    "      acc = acc.append(tmp)\n",
    "\n",
    "  claims_labels_dict = {}\n",
    "  for _, row in acc.iterrows():\n",
    "    if row[\"#Supports\"] > row[\"#Refutes\"]:\n",
    "      claims_labels_dict[\"\".join(nltk.word_tokenize(row[\"Claim\"]))] = True\n",
    "    else:\n",
    "      claims_labels_dict[\"\".join(nltk.word_tokenize(row[\"Claim\"]))] = False\n",
    "\n",
    "  return claims_labels_dict\n",
    "\n",
    "def get_pred_labels(model, test_df, pad_test_claim, pad_test_evidence):\n",
    "    predictions = model.predict((pad_test_claim, pad_test_evidence))\n",
    "    claims_labels_dict = {}\n",
    "    claims_flat = []\n",
    "    for claim in test_df['Claim']:\n",
    "      claim = claim.replace(\" \", \"\")\n",
    "      claim = claim.replace(\"\\\"\", \"\")\n",
    "      claim = claim.replace(\"\\'\", \"\")\n",
    "      if claim:\n",
    "        claims_flat.append(''.join(claim))\n",
    "\n",
    "    for index, claim in enumerate(claims_flat):\n",
    "      if claim in claims_labels_dict:\n",
    "        if predictions[index]:\n",
    "          claims_labels_dict[claim] = (claims_labels_dict[claim][0],\n",
    "                                        claims_labels_dict[claim][1] + 1)\n",
    "        else:\n",
    "          claims_labels_dict[claim] = (claims_labels_dict[claim][0] + 1,\n",
    "                                        claims_labels_dict[claim][1])\n",
    "      else:\n",
    "        if predictions[index]:\n",
    "          claims_labels_dict[claim] = (0, 1)\n",
    "        else:\n",
    "          claims_labels_dict[claim] = (1, 0)\n",
    "                                       \n",
    "    for claim, votes in claims_labels_dict.items():\n",
    "      if votes[0] > votes[1]:\n",
    "        claims_labels_dict[claim] = False\n",
    "      else:\n",
    "        claims_labels_dict[claim] = True\n",
    "    return claims_labels_dict\n",
    "\n",
    "def compute_metrics_majority_voting(true, pred):\n",
    "  true_labels = []\n",
    "  pred_labels = []\n",
    "  for claim, label in true.items():\n",
    "    if claim in pred:\n",
    "      true_labels.append(label)\n",
    "      claim = claim.replace(\"`\", \"\")\n",
    "      claim = claim.replace(\"\\'\", \"\")\n",
    "      pred_labels.append(pred[claim])\n",
    "  print(\"Accuracy:  \", accuracy_score(true_labels, pred_labels))\n",
    "  print(\"F1 score:  \", f1_score(true_labels, pred_labels))\n",
    "  print(\"Precision: \", precision_score(true_labels, pred_labels))\n",
    "  print(\"Recall:    \", recall_score(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Y175cow-Zmwr"
   },
   "outputs": [],
   "source": [
    "trues = get_true_labels(test_df)\n",
    "preds = get_pred_labels(model, test_df, pad_test_claim, pad_test_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ir39GlJZmwr",
    "outputId": "7d45c757-2e2b-4124-f730-1f5f06aefdb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:   0.5000846740050804\n",
      "F1 score:   0.6667419282004967\n",
      "Precision:  0.5000846740050804\n",
      "Recall:     1.0\n"
     ]
    }
   ],
   "source": [
    "compute_metrics_majority_voting(trues, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZEIxwNZZmwr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBKjtPR-Zmwr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzxK6rQRC0pC"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lcB4gMa550_"
   },
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeaKEOpyesqA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example 4\n",
    "id1 = 0\n",
    "#id1 = 78061\n",
    "#id1 = 7612\n",
    "v1 = sent_emb(pad_sequences(enc_train_df['Claim'], maxlen=max_length, padding='post')[id1].reshape(1, -1))\n",
    "v2 = sent_emb(pad_sequences(enc_train_df['Evidence'], maxlen=max_length, padding='post')[id1].reshape(1, -1))\n",
    "sim = cosine_similarity(v1, v2)\n",
    "print(f'similarity between \\\"{train_df[\"Claim\"][id1]}\\\" and \\\"{train_df[\"Evidence\"][id1]}\\\":')\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1dtoIXR2jkj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example 1\n",
    "id1 = 7612\n",
    "id2 = 78061\n",
    "v1 = sent_emb(pad_sequences(enc_train_df['Claim'], padding='post')[id1].reshape(1, -1))\n",
    "v2 = sent_emb(pad_sequences(enc_train_df['Claim'], padding='post')[id2].reshape(1, -1))\n",
    "sim = cosine_similarity(v1, v2)\n",
    "print(f'similarity between \\\"{train_df[\"Claim\"][id1]}\\\" and \\\"{train_df[\"Claim\"][id2]}\\\":')\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMMPsVr_4hNy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example 2\n",
    "id1 = 1\n",
    "id2 = 2\n",
    "v1 = sent_emb(pad_sequences(enc_train_df['Claim'], padding='post')[id1].reshape(1, -1))\n",
    "v2 = sent_emb(pad_sequences(enc_train_df['Claim'], padding='post')[id2].reshape(1, -1))\n",
    "sim = cosine_similarity(v1, v2)\n",
    "print(f'similarity between \\\"{train_df[\"Claim\"][id1]}\\\" and \\\"{train_df[\"Claim\"][id2]}\\\":')\n",
    "print(sim)\n",
    "#embedding_layer(pad_sequences(enc_train_df['Claim'], padding='post')[0].reshape(1, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkSBdwbaJEjg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example 3\n",
    "id1 = 0\n",
    "id2 = 3\n",
    "v1 = sent_emb(pad_sequences(enc_train_df['Claim'], padding='post')[id1].reshape(1, -1))\n",
    "v2 = sent_emb(pad_sequences(enc_train_df['Claim'], padding='post')[id2].reshape(1, -1))\n",
    "sim = cosine_similarity(v1, v2)\n",
    "print(f'similarity between \\\"{train_df[\"Claim\"][id1]}\\\" and \\\"{train_df[\"Claim\"][id2]}\\\":')\n",
    "print(sim)\n",
    "#embedding_layer(pad_sequences(enc_train_df['Claim'], padding='post')[0].reshape(1, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5j1ow_NdLOi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "glove.similarity('american', 'swedish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8Ha0LFP6Sy2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(enc_train_df[\"Evidence\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uozkFV_HdGs",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "train_df[train_df[\"Evidence\"].str.contains(\"Swift\")]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Uf80UVRNrXve",
    "COXeCXdYsBEf",
    "Ejv6SDE8xc4_",
    "TnEY9yI3y0aU",
    "VY8kXXkECfun",
    "dzxK6rQRC0pC",
    "-lcB4gMa550_"
   ],
   "name": "Assignment_4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
