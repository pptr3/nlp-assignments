{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GM9DBN-Qz3k"
   },
   "source": [
    "# Assignment 4\n",
    "\n",
    "**Due to**: TBD\n",
    "\n",
    "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
    "\n",
    "**Summary**: Fact checking, Neural Languange Inference (**NLI**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO_-4CZeRCO7"
   },
   "source": [
    "# Intro\n",
    "\n",
    "This assignment is centred on a particular and emerging NLP task, formally known as **fact checking** (or fake checking). As AI techniques become more and more powerful, reaching amazing results, such as image and text generation, it is more than ever necessary to build tools able to distinguish what is real from what is fake.\n",
    "\n",
    "Here we focus on a small portion of the whole fact checking problem, which aims to determine whether a given statement (fact) conveys a trustworthy information or not. \n",
    "\n",
    "More precisely, given a set of evidences and a fact to verify, we would like our model to correctly predict whether the fact is true or fake.\n",
    "\n",
    "In particular, we will see:\n",
    "\n",
    "*   Dataset preparation (analysis and pre-processing)\n",
    "*   Problem formulation: multi-input binary classification\n",
    "*   Defining an evaluation method\n",
    "*   Simple sentence embedding\n",
    "*   Neural building blocks\n",
    "*   Neural architecture extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGDwg78PS_uy"
   },
   "source": [
    "# The FEVER dataset\n",
    "\n",
    "First of all, we need to choose a dataset. In this assignment we will rely on the [FEVER dataset](https://fever.ai).\n",
    "\n",
    "The dataset is about facts taken from Wikipedia documents that have to be verified. In particular, facts could face manual modifications in order to define fake information or to give different formulations of the same concept.\n",
    "\n",
    "The dataset consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as ```Supported```, ```Refuted``` or ```NotEnoughInfo```. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Oa5FpVpT7p4"
   },
   "source": [
    "## Dataset structure\n",
    "\n",
    "Relevant data is divided into two file types. Information concerning the fact to verify, its verdict and associated supporting/opposing statements are stored in **.jsonl** format. In particular, each JSON element is a python dictionary with the following relevant fields:\n",
    "\n",
    "*    **ID**: ID associated to the fact to verify.\n",
    "\n",
    "*    **Verifiable**: whether the fact has been verified or not: ```VERIFIABLE``` (or ```NOT VERIFIABLE```).\n",
    "    \n",
    "*    **Label**: the final verdict on the fact to verify: ```SUPPORTS```, ```REFUTES``` (or ```NOT ENOUGH INFO```).\n",
    "    \n",
    "*    **Claim**: the fact to verify.\n",
    "    \n",
    "*    **Evidence**: a nested list of document IDs along with the sentence ID that is associated to the fact to verify. In particular, each list element is a tuple of four elements: the first two are internal annotator IDs that can be safely ignored; the third term is the document ID (called URL) and the last one is the sentence number (ID) in the pointed document to consider.\n",
    "\n",
    "**Some Examples**\n",
    "\n",
    "---\n",
    "\n",
    "**Verifiable**\n",
    "\n",
    "{\"id\": 202314, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"The New Jersey Turnpike has zero shoulders.\", \"evidence\": [[[238335, 240393, \"New_Jersey_Turnpike\", 15]]]}\n",
    "\n",
    "---\n",
    "\n",
    "**Not Verifiable**\n",
    "\n",
    "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nex_8UM4VWuY"
   },
   "source": [
    "## Some simplifications and pre-processing\n",
    "\n",
    "We are only interested in verifiable facts. Thus, we can filter out all non-verifiable claims.\n",
    "\n",
    "Additionally, the current dataset format does not contain all necessary information for our classification purposes. In particular, we need to download Wikipedia documents and replace reported evidence IDs with the corresponding text.\n",
    "\n",
    "Don't worry about that! We are providing you the already pre-processed dataset so that you can concentrate on the classification pipeline (pre-processing, model definition, evaluation and training).\n",
    "\n",
    "You can download the zip file containing all set splits (train, validation and test) of the FEVER dataset by clicking on this [link](https://drive.google.com/file/d/1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1/view?usp=sharing). Alternatively, run the below code cell to automatically download it on this notebook.\n",
    "\n",
    "**Note**: each dataset split is in .csv format. Feel free to inspect the whole dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BspxZcRjW0NG",
    "outputId": "23dd2947-797f-4d0a-e307-7666d1e2e7ae"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_path):\n",
    "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
    "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
    "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(toy_data_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:\n",
    "            response = current_session.get(toy_url,\n",
    "                                   params={'id': toy_data_url_id},\n",
    "                                   stream=True)\n",
    "        save_response_content(response, toy_data_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
    "            loaded_zip.extractall(data_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "download_data('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbH_8errW5MH"
   },
   "source": [
    "# Classification dataset\n",
    "\n",
    "At this point, you should have a reay-to-go dataset! Note that the dataset format changed as well! In particular, we split the evidence set associated to each claim, in order to build (claim, evidence) pairs. The classification label is propagated as well.\n",
    "\n",
    "We'll motivate this decision in the next section!\n",
    "\n",
    "Just for clarity, here's an example of the pre-processed dataset:\n",
    "\n",
    "---\n",
    "\n",
    "**Claim**: \"Wentworth Miller is yet to make his screenwriting debut.\"\n",
    "\n",
    "**Evidence**: \"2\tHe made his screenwriting debut with the 2013 thriller film Stoker .\tStoker\tStoker (film)\"\n",
    "\n",
    "**Label**: Refutes\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: The dataset requires some text cleaning as you may notice!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gH8hIK21Xrl0"
   },
   "source": [
    "# Problem formulation\n",
    "\n",
    "As mentioned at the beginning of the assignment, we are going to formulate the fact checking problem as a binary classification task.\n",
    "\n",
    "In particular, each dataset sample is comprised of:\n",
    "\n",
    "*     A claim to verify\n",
    "*     A set of semantically related statements (evidence set)\n",
    "*     Fact checking label: either evidences support or refute the claim.\n",
    "\n",
    "Handling the evidence set from the point of view of neural models may imply some additional complexity: if the evidence set is comprised of several sentences we might incur in memory problems.\n",
    "\n",
    "To this end, we further simplify the problem by building (claim, evidence) pairs. The fact checking label is propagated as well.\n",
    "\n",
    "Example:\n",
    "\n",
    "     Claim: c1 \n",
    "     Evidence set: [e1, e2, e3]\n",
    "     Label: S (support)\n",
    "\n",
    "--->\n",
    "\n",
    "    (c1, e1, S),\n",
    "    (c1, e2, S),\n",
    "    (c1, e3, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E46flIz_zQy-"
   },
   "source": [
    "## Schema\n",
    "\n",
    "The overall binary classification problem is summed up by the following (simplified) schema\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Wm_YBnFwgJtxcWEBpPbTBEVkpKaL08Jp)\n",
    "\n",
    "Don't worry too much about the **Encoding** block for now. We'll give you some simple guidelines about its definition. For the moment, stick to the binary classification task definition where, in this case, we have 2 inputs: the claim to verify and one of its associated evidences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsiTV-NVdgsF"
   },
   "source": [
    "# Architecture Guidelines\n",
    "\n",
    "There are many neural architectures that follow the above schema. To avoid phenomena like the writer's block, in this section we are going to give you some implementation guidelines.\n",
    "\n",
    "In particular, we would like you to test some implementations so that you explore basic approaches (neural baselines) and use them as building blocks for possible extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJBQm47fe7iE"
   },
   "source": [
    "## Handling multiple inputs\n",
    "\n",
    "The first thing to notice is that we are in a multi-input scenario. In particular, each sample is comprised of a fact and its asssociated evidence statement.\n",
    "\n",
    "Each of these input is encoded as a sequence of tokens. In particular, we will have the following input matrices:\n",
    "\n",
    "*    Claim: [batch_size, max_tokens]\n",
    "*    Evidence: [batch_size, max_tokens]\n",
    "\n",
    "Moreover, after the embedding layer, we'll have:\n",
    "\n",
    "*    Claim: [batch_size, max_tokens, embedding_dim]\n",
    "*    Evidence: [batch_size, max_tokens, embedding_dim]\n",
    "\n",
    "But, we would like to have a 2D input to our classifier, since we have to give an answer at pair level. Therefore, for each sample, we would expect the following input shape to our classification block:\n",
    "\n",
    "*   Classification input shape: [batch_size, dim]\n",
    "\n",
    "**How to do that?**\n",
    "\n",
    "We inherently need to reduce the token sequence to a single representation. This operation is formally known as **sentence embedding**. Indeed, we are trying to compress the information of a whole sequence into a single embedding vector.\n",
    "\n",
    "Here are some simple solutions that we ask you to try out:\n",
    "\n",
    "*   Encode token sequences via a RNN and take the last state as the sentence embedding.\n",
    "\n",
    "*   Encode token sequences via a RNN and average all the output states.\n",
    "\n",
    "*   Encode token sequences via a simple MLP layer. In particular, if your input is a [batch_size, max_tokens, embedding_dim] tensor, the matrix multiplication works on the **max_tokens** dimension, resulting in a [batch_size, embedding_dim] 2D matrix.\n",
    "\n",
    "*   Compute the sentence embedding as the mean of its token embeddings (**bag of vectors**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/train_pairs.csv')\n",
    "val_df = pd.read_csv('dataset/val_pairs.csv')\n",
    "test_df = pd.read_csv('dataset/test_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Chris Hemsworth appeared in A Perfect Getaway.</td>\n",
       "      <td>2\\tHemsworth has also appeared in the science ...</td>\n",
       "      <td>3</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Roald Dahl is a writer.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Roald Dahl is a governor.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
       "      <td>8</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ireland has relatively low-lying mountains.</td>\n",
       "      <td>10\\tThe island 's geography comprises relative...</td>\n",
       "      <td>9</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>10\\tThe island 's geography comprises relative...</td>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Claim  \\\n",
       "0           0     Chris Hemsworth appeared in A Perfect Getaway.   \n",
       "1           1                            Roald Dahl is a writer.   \n",
       "2           2                          Roald Dahl is a governor.   \n",
       "3           3        Ireland has relatively low-lying mountains.   \n",
       "4           4  Ireland does not have relatively low-lying mou...   \n",
       "\n",
       "                                            Evidence  ID     Label  \n",
       "0  2\\tHemsworth has also appeared in the science ...   3  SUPPORTS  \n",
       "1  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   7  SUPPORTS  \n",
       "2  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   8   REFUTES  \n",
       "3  10\\tThe island 's geography comprises relative...   9  SUPPORTS  \n",
       "4  10\\tThe island 's geography comprises relative...  10   REFUTES  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing text...\n",
      "Pre-processing completed!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Config\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def replace_special_characters(text):\n",
    "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "def filter_out_uncommon_symbols(text):\n",
    "    return GOOD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
    "\n",
    "def strip_text(text):\n",
    "    return text.strip()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "PREPROCESSING_PIPELINE = [\n",
    "                          lower,\n",
    "                          replace_special_characters,\n",
    "                          filter_out_uncommon_symbols,\n",
    "                          remove_stopwords,\n",
    "                          strip_text,\n",
    "                          remove_numbers\n",
    "                          ]\n",
    "\n",
    "\n",
    "def text_prepare(text, filter_methods=None):\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
    "\n",
    "# Pre-processing\n",
    "print('Pre-processing text...')\n",
    "# Replace each sentence with its pre-processed version\n",
    "train_df['Claim'] = train_df['Claim'].apply(lambda txt: text_prepare(txt))\n",
    "train_df['Evidence'] = train_df['Evidence'].apply(lambda txt: text_prepare(txt))\n",
    "\n",
    "val_df['Claim'] = val_df['Claim'].apply(lambda txt: text_prepare(txt))\n",
    "val_df['Evidence'] = val_df['Evidence'].apply(lambda txt: text_prepare(txt))\n",
    "\n",
    "test_df['Claim'] = val_df['Claim'].apply(lambda txt: text_prepare(txt))\n",
    "test_df['Evidence'] = val_df['Evidence'].apply(lambda txt: text_prepare(txt))\n",
    "\n",
    "print(\"Pre-processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voc(df):\n",
    "    voc_inverted = {}\n",
    "    current_index = 0\n",
    "    df_text = np.array(df)\n",
    "    for i in range(df_text.shape[0]):\n",
    "        df_text_split = df_text[i].split(' ')\n",
    "        for word in df_text_split:\n",
    "            if word not in voc_inverted:\n",
    "                voc_inverted[word] = current_index\n",
    "                current_index += 1\n",
    "    \n",
    "    voc = {v: k for k, v in voc_inverted.items()}\n",
    "    \n",
    "    return voc, voc_inverted, voc_inverted.keys()\n",
    "# idx_to_word, word_to_idx, word_listing\n",
    "train_voc_claim = build_voc(train_df['Claim'])\n",
    "train_voc_evidence = build_voc(train_df['Evidence'])\n",
    "\n",
    "val_voc_claim = build_voc(val_df['Claim'])\n",
    "val_voc_evidence = build_voc(val_df['Evidence'])\n",
    "\n",
    "#test_voc_claim = build_voc(test_df['Claim'])\n",
    "#test_voc_evidence = build_voc(test_df['Evidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(model_type, embedding_dimension=300):\n",
    "    download_path = \"\"\n",
    "\n",
    "    # Find the correct embedding model name\n",
    "    if model_type.strip().lower() == 'word2vec':\n",
    "        download_path = \"word2vec-google-news-300\"\n",
    "\n",
    "    elif model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove\")\n",
    "\n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model\n",
    "\n",
    "embedding_model_type = \"glove\"\n",
    "embedding_dimension = 50\n",
    "\n",
    "embedding_model = load_embedding_model(embedding_model_type, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model, word_listing):\n",
    "    model_vocab_array = np.array(list(embedding_model.vocab.keys()))\n",
    "    word_listing_array = np.array(list(word_listing))\n",
    "    return list(np.setdiff1d(word_listing_array, model_vocab_array))\n",
    "# claim\n",
    "train_oov_terms_claim = check_OOV_terms(embedding_model, train_voc_claim[2]) # voc_claim[2] represents\n",
    "                                                                       # the word listing\n",
    "# evidence\n",
    "train_oov_terms_evidence = check_OOV_terms(embedding_model, train_voc_evidence[2])\n",
    "\n",
    "\n",
    "# claim\n",
    "val_oov_terms_claim = check_OOV_terms(embedding_model, val_voc_claim[2])\n",
    "# evidence\n",
    "val_oov_terms_evidence = check_OOV_terms(embedding_model, val_voc_evidence[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, oov_terms):\n",
    "    words = np.array(list(word_to_idx.keys()))\n",
    "    embedding_matrix = np.zeros((len(words), embedding_dimension))    \n",
    "    for idx, w in enumerate(words):\n",
    "        if w not in oov_terms:\n",
    "            embedding_matrix[idx, :] = np.array(embedding_model.wv[w])\n",
    "        else:\n",
    "           # random vectors\n",
    "            mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "            embedding_matrix[idx, :] = np.random.normal(mu, sigma, embedding_dimension)\n",
    "    return embedding_matrix\n",
    "train_embedding_matrix_claim = build_embedding_matrix(embedding_model, embedding_dimension, train_voc_claim[1], train_oov_terms_claim)\n",
    "train_embedding_matrix_evidence = build_embedding_matrix(embedding_model, embedding_dimension, train_voc_evidence[1], train_oov_terms_evidence)\n",
    "\n",
    "val_embedding_matrix_claim = build_embedding_matrix(embedding_model, embedding_dimension, val_voc_claim[1], val_oov_terms_claim)\n",
    "val_embedding_matrix_evidence = build_embedding_matrix(embedding_model, embedding_dimension, val_voc_evidence[1], val_oov_terms_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3D_matrix(train_df, voc_df, max_length, embedding_matrix, embedding_dimension):\n",
    "    train_df_array = np.array(train_df)\n",
    "    # zero padding\n",
    "    three_dim_matrix = np.zeros((train_df_array.shape[0], max_length, embedding_dimension))\n",
    "    for i in range(train_df_array.shape[0]):\n",
    "        df_claim_row = train_df_array[i].split(' ')\n",
    "        for idx, word in enumerate(df_claim_row):\n",
    "            three_dim_matrix[i, idx, :] = embedding_matrix[voc_df[1][word]]\n",
    "    return three_dim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_dim_matrix_claim = create_3D_matrix(train_df['Claim'], voc_claim, max_length, embedding_matrix_claim, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_dim_matrix_evidence = create_3D_matrix(train_df['Evidence'], voc_evidence, max_length, embedding_matrix_evidence, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121740, 150, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_dim_matrix_claim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_dim_matrix_claim[0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121740, 150, 50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_dim_matrix_evidence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Bidirectional, \\\n",
    "                                    LSTM, GRU, SimpleRNN, Lambda, TimeDistributed, \\\n",
    "                                    Masking, GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trining data\n",
    "word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
    "word_tokenizer.fit_on_texts(train_df['Claim'])    # fit tokeniser on data\n",
    "train_df_claim_enc = word_tokenizer.texts_to_sequences(train_df['Claim'])\n",
    "\n",
    "word_tokenizer = Tokenizer()                      \n",
    "word_tokenizer.fit_on_texts(train_df['Evidence'])\n",
    "train_df_evidence_enc = word_tokenizer.texts_to_sequences(train_df['Evidence'])\n",
    "\n",
    "# for val data\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(val_df['Claim'])\n",
    "val_df_claim_enc = word_tokenizer.texts_to_sequences(val_df['Claim'])\n",
    "\n",
    "word_tokenizer = Tokenizer()                      \n",
    "word_tokenizer.fit_on_texts(val_df['Evidence'])\n",
    "val_df_evidence_enc = word_tokenizer.texts_to_sequences(val_df['Evidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_layer_claim = Embedding(\n",
    "    train_embedding_matrix_claim.shape[0],    # vocab size \n",
    "    train_embedding_matrix_claim.shape[1],    # embedding dimension\n",
    "    embeddings_initializer=tf.initializers.Constant(train_embedding_matrix_claim),\n",
    "    mask_zero = True,\n",
    "    name = 'Embedding_claim',\n",
    "    trainable = False\n",
    ")\n",
    "\n",
    "train_embedd_sentences_claim = Sequential([\n",
    "  Input((max_length,)),\n",
    "  train_embedding_layer_claim,\n",
    "  Bidirectional(LSTM(64, dropout=0.1, return_sequences=True)),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(max_length),\n",
    "])\n",
    "\n",
    "train_embedding_layer_evidence = Embedding(\n",
    "    train_embedding_matrix_evidence.shape[0],    # vocab size \n",
    "    train_embedding_matrix_evidence.shape[1],    # embedding dimension\n",
    "    embeddings_initializer=tf.initializers.Constant(train_embedding_matrix_evidence),\n",
    "    mask_zero = True,\n",
    "    name = 'Embedding_evidence',\n",
    "    trainable = False\n",
    ")\n",
    "\n",
    "train_embedd_sentences_evidence = Sequential([\n",
    "  Input((max_length,)),\n",
    "  train_embedding_layer_evidence,\n",
    "  Bidirectional(LSTM(64, dropout=0.1, return_sequences=True)),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(max_length),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embedding_layer_claim = Embedding(\n",
    "    val_embedding_matrix_claim.shape[0],    # vocab size \n",
    "    val_embedding_matrix_claim.shape[1],    # embedding dimension\n",
    "    embeddings_initializer=tf.initializers.Constant(val_embedding_matrix_claim),\n",
    "    mask_zero = True,\n",
    "    name = 'Embedding_claim',\n",
    "    trainable = False\n",
    ")\n",
    "\n",
    "val_embedd_sentences_claim = Sequential([\n",
    "  Input((max_length,)),\n",
    "  val_embedding_layer_claim,\n",
    "  Bidirectional(LSTM(64, dropout=0.1, return_sequences=True)),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(max_length),\n",
    "])\n",
    "\n",
    "val_embedding_layer_evidence = Embedding(\n",
    "    val_embedding_matrix_evidence.shape[0],    # vocab size \n",
    "    val_embedding_matrix_evidence.shape[1],    # embedding dimension\n",
    "    embeddings_initializer=tf.initializers.Constant(val_embedding_matrix_evidence),\n",
    "    mask_zero = True,\n",
    "    name = 'Embedding_evidence',\n",
    "    trainable = False\n",
    ")\n",
    "\n",
    "val_embedd_sentences_evidence = Sequential([\n",
    "  Input((max_length,)),\n",
    "  val_embedding_layer_evidence,\n",
    "  Bidirectional(LSTM(64, dropout=0.1, return_sequences=True)),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(max_length),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedd all claim matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length_df = len(train_df.index)\n",
    "length_df = 100 # for testing use just 100\n",
    "train_claim_2D = np.zeros((length_df, max_length))\n",
    "for i in range(length_df):\n",
    "    train_claim_2D[i, :] = train_embedd_sentences_claim(pad_sequences(train_df_claim_enc, padding='post')[i].reshape(1, -1))\n",
    "    \n",
    "train_evidence_2D = np.zeros((length_df, max_length))\n",
    "for i in range(length_df):\n",
    "    train_evidence_2D[i, :] = train_embedd_sentences_evidence(pad_sequences(train_df_evidence_enc, padding='post')[i].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length_df = len(train_df.index)\n",
    "length_df = 100 # for testing use just 100\n",
    "val_claim_2D = np.zeros((length_df, max_length))\n",
    "for i in range(length_df):\n",
    "    val_claim_2D[i, :] = val_embedd_sentences_claim(pad_sequences(val_df_claim_enc, padding='post')[i].reshape(1, -1))\n",
    "    \n",
    "val_evidence_2D = np.zeros((length_df, max_length))\n",
    "for i in range(length_df):\n",
    "    val_evidence_2D[i, :] = val_embedd_sentences_evidence(pad_sequences(val_df_evidence_enc, padding='post')[i].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gypl5z5ElJo1"
   },
   "source": [
    "## Merging multi-inputs\n",
    "\n",
    "At this point, we have to think about **how** we should merge evidence and claim sentence embeddings.\n",
    "\n",
    "For simplicity, we stick to simple merging strategies:\n",
    "\n",
    "*     **Concatenation**: define the classification input as the concatenation of evidence and claim sentence embeddings\n",
    "\n",
    "*     **Sum**: define the classification input as the sum of evidence and claim sentence embeddings\n",
    "\n",
    "*     **Mean**: define the classification input as the mean of evidence and claim sentence embeddings\n",
    "\n",
    "For clarity, if we the sentence embedding of a single input has shape [batch_size, embedding_dim], then the classification input has shape:\n",
    "\n",
    "*     **Concatenation**: [batch_size, 2 * embedding_dim]\n",
    "\n",
    "*     **Sum**: [batch_size, embedding_dim]\n",
    "\n",
    "*     **Mean**: [batch_size, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claim_evidence_concat = np.hstack((train_claim_2D, train_evidence_2D))\n",
    "val_claim_evidence_concat = np.hstack((val_claim_2D, val_evidence_2D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 300)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_claim_evidence_concat.shape, val_claim_evidence_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Label\"] = train_df[\"Label\"].apply(lambda x: 1 if x==\"SUPPORTS\" else 0)\n",
    "val_df[\"Label\"] = val_df[\"Label\"].apply(lambda x: 1 if x==\"SUPPORTS\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df[\"Label\"])\n",
    "y_val = np.array(val_df[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=train_claim_evidence_concat.shape[1], activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "100/100 [==============================] - 0s 239us/sample - loss: 0.5439 - accuracy: 0.9100\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 0s 121us/sample - loss: 0.4990 - accuracy: 0.9100\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 0s 246us/sample - loss: 0.4467 - accuracy: 0.9100\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 0s 257us/sample - loss: 0.4076 - accuracy: 0.9100\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 0s 233us/sample - loss: 0.3791 - accuracy: 0.9100\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 0s 319us/sample - loss: 0.3526 - accuracy: 0.9100\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 0s 200us/sample - loss: 0.3392 - accuracy: 0.9100\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 0s 200us/sample - loss: 0.3273 - accuracy: 0.9100\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 0s 185us/sample - loss: 0.3186 - accuracy: 0.9100\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 0s 263us/sample - loss: 0.3129 - accuracy: 0.9100\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 0s 239us/sample - loss: 0.3093 - accuracy: 0.9100\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 0s 184us/sample - loss: 0.3047 - accuracy: 0.9100\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 0s 255us/sample - loss: 0.3012 - accuracy: 0.9100\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 0s 216us/sample - loss: 0.2981 - accuracy: 0.9100\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 0s 226us/sample - loss: 0.2949 - accuracy: 0.9100\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 0s 220us/sample - loss: 0.2922 - accuracy: 0.9100\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 0s 204us/sample - loss: 0.2898 - accuracy: 0.9100\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 0s 280us/sample - loss: 0.2866 - accuracy: 0.9100\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 0s 244us/sample - loss: 0.2830 - accuracy: 0.9100\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 0s 232us/sample - loss: 0.2804 - accuracy: 0.9100\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 0s 252us/sample - loss: 0.2772 - accuracy: 0.9100\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 0s 305us/sample - loss: 0.2741 - accuracy: 0.9100\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 0s 256us/sample - loss: 0.2713 - accuracy: 0.9100\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 0s 190us/sample - loss: 0.2681 - accuracy: 0.9100\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 0s 236us/sample - loss: 0.2654 - accuracy: 0.9100\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 0s 274us/sample - loss: 0.2612 - accuracy: 0.9100\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 0s 253us/sample - loss: 0.2581 - accuracy: 0.9100\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 0s 248us/sample - loss: 0.2544 - accuracy: 0.9100\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 0s 285us/sample - loss: 0.2510 - accuracy: 0.9100\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 0s 188us/sample - loss: 0.2476 - accuracy: 0.9100\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 0s 254us/sample - loss: 0.2438 - accuracy: 0.9100\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 0s 268us/sample - loss: 0.2401 - accuracy: 0.9100\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 0s 189us/sample - loss: 0.2360 - accuracy: 0.9100\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 0s 228us/sample - loss: 0.2342 - accuracy: 0.9100\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 0s 226us/sample - loss: 0.2299 - accuracy: 0.9200\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 0s 245us/sample - loss: 0.2253 - accuracy: 0.9200\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 0s 244us/sample - loss: 0.2222 - accuracy: 0.9200\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 0s 223us/sample - loss: 0.2211 - accuracy: 0.9200\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 0s 243us/sample - loss: 0.2153 - accuracy: 0.9200\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 0s 220us/sample - loss: 0.2137 - accuracy: 0.9200\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 0s 193us/sample - loss: 0.2099 - accuracy: 0.9200\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 0s 236us/sample - loss: 0.2069 - accuracy: 0.9200\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 0s 259us/sample - loss: 0.2033 - accuracy: 0.9200\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 0s 245us/sample - loss: 0.2008 - accuracy: 0.9200\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 0s 262us/sample - loss: 0.1975 - accuracy: 0.9200\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 0s 223us/sample - loss: 0.1944 - accuracy: 0.9200\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 0s 258us/sample - loss: 0.1919 - accuracy: 0.9200\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 0s 202us/sample - loss: 0.1882 - accuracy: 0.9200\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 0s 292us/sample - loss: 0.1876 - accuracy: 0.9200\n",
      "Epoch 50/200\n",
      "100/100 [==============================] - 0s 173us/sample - loss: 0.1827 - accuracy: 0.9200\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 0s 230us/sample - loss: 0.1797 - accuracy: 0.9200\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 0s 210us/sample - loss: 0.1770 - accuracy: 0.9200\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 0s 267us/sample - loss: 0.1742 - accuracy: 0.9200\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 0s 270us/sample - loss: 0.1725 - accuracy: 0.9200\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 0s 236us/sample - loss: 0.1699 - accuracy: 0.9200\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 0s 245us/sample - loss: 0.1675 - accuracy: 0.9200\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 0s 233us/sample - loss: 0.1646 - accuracy: 0.9300\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 0s 255us/sample - loss: 0.1619 - accuracy: 0.9300\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 0s 232us/sample - loss: 0.1591 - accuracy: 0.9300\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 0s 255us/sample - loss: 0.1566 - accuracy: 0.9400\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 0s 260us/sample - loss: 0.1552 - accuracy: 0.9400\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 0s 259us/sample - loss: 0.1523 - accuracy: 0.9400\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 0s 246us/sample - loss: 0.1498 - accuracy: 0.9400\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 0s 433us/sample - loss: 0.1493 - accuracy: 0.9400\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 0s 271us/sample - loss: 0.1459 - accuracy: 0.9400\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 0s 286us/sample - loss: 0.1438 - accuracy: 0.9400\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 0s 288us/sample - loss: 0.1439 - accuracy: 0.9400\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 0s 243us/sample - loss: 0.1394 - accuracy: 0.9400\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 0s 249us/sample - loss: 0.1388 - accuracy: 0.9400\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 0s 187us/sample - loss: 0.1363 - accuracy: 0.9400\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 0s 281us/sample - loss: 0.1343 - accuracy: 0.9400\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 0s 254us/sample - loss: 0.1328 - accuracy: 0.9400\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 0s 229us/sample - loss: 0.1306 - accuracy: 0.9400\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 0s 218us/sample - loss: 0.1285 - accuracy: 0.9400\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 0s 235us/sample - loss: 0.1273 - accuracy: 0.9400\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - 0s 197us/sample - loss: 0.1260 - accuracy: 0.9500\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 0s 261us/sample - loss: 0.1238 - accuracy: 0.9500\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 200us/sample - loss: 0.1235 - accuracy: 0.9500\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 0s 184us/sample - loss: 0.1207 - accuracy: 0.9500\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 0s 197us/sample - loss: 0.1201 - accuracy: 0.9500\n",
      "Epoch 81/200\n",
      "100/100 [==============================] - 0s 192us/sample - loss: 0.1185 - accuracy: 0.9500\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 0s 211us/sample - loss: 0.1171 - accuracy: 0.9500\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - 0s 231us/sample - loss: 0.1154 - accuracy: 0.9500\n",
      "Epoch 84/200\n",
      "100/100 [==============================] - 0s 181us/sample - loss: 0.1142 - accuracy: 0.9500\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - 0s 203us/sample - loss: 0.1124 - accuracy: 0.9500\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - 0s 227us/sample - loss: 0.1129 - accuracy: 0.9500\n",
      "Epoch 87/200\n",
      "100/100 [==============================] - 0s 184us/sample - loss: 0.1098 - accuracy: 0.9500\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - 0s 201us/sample - loss: 0.1138 - accuracy: 0.9400\n",
      "Epoch 89/200\n",
      "100/100 [==============================] - 0s 213us/sample - loss: 0.1073 - accuracy: 0.9400\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - 0s 187us/sample - loss: 0.1067 - accuracy: 0.9500\n",
      "Epoch 91/200\n",
      "100/100 [==============================] - 0s 200us/sample - loss: 0.1074 - accuracy: 0.9500\n",
      "Epoch 92/200\n",
      "100/100 [==============================] - 0s 190us/sample - loss: 0.1044 - accuracy: 0.9500\n",
      "Epoch 93/200\n",
      "100/100 [==============================] - 0s 154us/sample - loss: 0.1033 - accuracy: 0.9500\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - 0s 191us/sample - loss: 0.1023 - accuracy: 0.9500\n",
      "Epoch 95/200\n",
      "100/100 [==============================] - 0s 178us/sample - loss: 0.1026 - accuracy: 0.9500\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - 0s 200us/sample - loss: 0.1007 - accuracy: 0.9500\n",
      "Epoch 97/200\n",
      "100/100 [==============================] - 0s 193us/sample - loss: 0.0996 - accuracy: 0.9500\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - 0s 225us/sample - loss: 0.0988 - accuracy: 0.9500\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - 0s 225us/sample - loss: 0.0986 - accuracy: 0.9500\n",
      "Epoch 100/200\n",
      "100/100 [==============================] - 0s 212us/sample - loss: 0.1019 - accuracy: 0.9400\n",
      "Epoch 101/200\n",
      "100/100 [==============================] - 0s 203us/sample - loss: 0.0955 - accuracy: 0.9500\n",
      "Epoch 102/200\n",
      "100/100 [==============================] - 0s 212us/sample - loss: 0.0947 - accuracy: 0.9600\n",
      "Epoch 103/200\n",
      "100/100 [==============================] - 0s 164us/sample - loss: 0.0957 - accuracy: 0.9600\n",
      "Epoch 104/200\n",
      "100/100 [==============================] - 0s 179us/sample - loss: 0.0957 - accuracy: 0.9500\n",
      "Epoch 105/200\n",
      "100/100 [==============================] - 0s 200us/sample - loss: 0.0959 - accuracy: 0.9600\n",
      "Epoch 106/200\n",
      "100/100 [==============================] - 0s 272us/sample - loss: 0.0925 - accuracy: 0.9600\n",
      "Epoch 107/200\n",
      "100/100 [==============================] - 0s 260us/sample - loss: 0.0912 - accuracy: 0.9600\n",
      "Epoch 108/200\n",
      "100/100 [==============================] - 0s 222us/sample - loss: 0.0900 - accuracy: 0.9600\n",
      "Epoch 109/200\n",
      "100/100 [==============================] - 0s 325us/sample - loss: 0.0900 - accuracy: 0.9600\n",
      "Epoch 110/200\n",
      "100/100 [==============================] - 0s 219us/sample - loss: 0.0889 - accuracy: 0.9600\n",
      "Epoch 111/200\n",
      "100/100 [==============================] - 0s 233us/sample - loss: 0.0878 - accuracy: 0.9600\n",
      "Epoch 112/200\n",
      "100/100 [==============================] - 0s 205us/sample - loss: 0.0877 - accuracy: 0.9600\n",
      "Epoch 113/200\n",
      "100/100 [==============================] - 0s 218us/sample - loss: 0.0866 - accuracy: 0.9600\n",
      "Epoch 114/200\n",
      "100/100 [==============================] - 0s 207us/sample - loss: 0.0866 - accuracy: 0.9600\n",
      "Epoch 115/200\n",
      "100/100 [==============================] - 0s 209us/sample - loss: 0.0854 - accuracy: 0.9600\n",
      "Epoch 116/200\n",
      "100/100 [==============================] - 0s 230us/sample - loss: 0.0846 - accuracy: 0.9600\n",
      "Epoch 117/200\n",
      "100/100 [==============================] - 0s 208us/sample - loss: 0.0841 - accuracy: 0.9600\n",
      "Epoch 118/200\n",
      "100/100 [==============================] - 0s 230us/sample - loss: 0.0835 - accuracy: 0.9600\n",
      "Epoch 119/200\n",
      "100/100 [==============================] - 0s 194us/sample - loss: 0.0833 - accuracy: 0.9600\n",
      "Epoch 120/200\n",
      "100/100 [==============================] - 0s 249us/sample - loss: 0.0827 - accuracy: 0.9600\n",
      "Epoch 121/200\n",
      "100/100 [==============================] - 0s 272us/sample - loss: 0.0829 - accuracy: 0.9600\n",
      "Epoch 122/200\n",
      "100/100 [==============================] - 0s 267us/sample - loss: 0.0818 - accuracy: 0.9600\n",
      "Epoch 123/200\n",
      "100/100 [==============================] - 0s 190us/sample - loss: 0.0830 - accuracy: 0.9500\n",
      "Epoch 124/200\n",
      "100/100 [==============================] - 0s 214us/sample - loss: 0.0801 - accuracy: 0.9500\n",
      "Epoch 125/200\n",
      "100/100 [==============================] - 0s 222us/sample - loss: 0.0812 - accuracy: 0.9600\n",
      "Epoch 126/200\n",
      "100/100 [==============================] - 0s 237us/sample - loss: 0.0796 - accuracy: 0.9500\n",
      "Epoch 127/200\n",
      "100/100 [==============================] - 0s 200us/sample - loss: 0.0791 - accuracy: 0.9600\n",
      "Epoch 128/200\n",
      "100/100 [==============================] - 0s 188us/sample - loss: 0.0787 - accuracy: 0.9600\n",
      "Epoch 129/200\n",
      "100/100 [==============================] - 0s 198us/sample - loss: 0.0787 - accuracy: 0.9500\n",
      "Epoch 130/200\n",
      "100/100 [==============================] - 0s 184us/sample - loss: 0.0798 - accuracy: 0.9600\n",
      "Epoch 131/200\n",
      "100/100 [==============================] - 0s 226us/sample - loss: 0.0782 - accuracy: 0.9500\n",
      "Epoch 132/200\n",
      "100/100 [==============================] - 0s 207us/sample - loss: 0.0784 - accuracy: 0.9400\n",
      "Epoch 133/200\n",
      "100/100 [==============================] - 0s 197us/sample - loss: 0.0767 - accuracy: 0.9600\n",
      "Epoch 134/200\n",
      "100/100 [==============================] - 0s 237us/sample - loss: 0.0758 - accuracy: 0.9600\n",
      "Epoch 135/200\n",
      "100/100 [==============================] - 0s 346us/sample - loss: 0.0764 - accuracy: 0.9500\n",
      "Epoch 136/200\n",
      "100/100 [==============================] - 0s 302us/sample - loss: 0.0759 - accuracy: 0.9500\n",
      "Epoch 137/200\n",
      "100/100 [==============================] - 0s 203us/sample - loss: 0.0749 - accuracy: 0.9600\n",
      "Epoch 138/200\n",
      "100/100 [==============================] - 0s 269us/sample - loss: 0.0747 - accuracy: 0.9600\n",
      "Epoch 139/200\n",
      "100/100 [==============================] - 0s 215us/sample - loss: 0.0737 - accuracy: 0.9500\n",
      "Epoch 140/200\n",
      "100/100 [==============================] - 0s 236us/sample - loss: 0.0743 - accuracy: 0.9500\n",
      "Epoch 141/200\n",
      "100/100 [==============================] - 0s 230us/sample - loss: 0.0729 - accuracy: 0.9600\n",
      "Epoch 142/200\n",
      "100/100 [==============================] - 0s 214us/sample - loss: 0.0735 - accuracy: 0.9500\n",
      "Epoch 143/200\n",
      "100/100 [==============================] - 0s 203us/sample - loss: 0.0721 - accuracy: 0.9600\n",
      "Epoch 144/200\n",
      "100/100 [==============================] - 0s 232us/sample - loss: 0.0734 - accuracy: 0.9600\n",
      "Epoch 145/200\n",
      "100/100 [==============================] - 0s 201us/sample - loss: 0.0722 - accuracy: 0.9600\n",
      "Epoch 146/200\n",
      "100/100 [==============================] - 0s 176us/sample - loss: 0.0715 - accuracy: 0.9600\n",
      "Epoch 147/200\n",
      "100/100 [==============================] - 0s 181us/sample - loss: 0.0709 - accuracy: 0.9600\n",
      "Epoch 148/200\n",
      "100/100 [==============================] - 0s 194us/sample - loss: 0.0726 - accuracy: 0.9400\n",
      "Epoch 149/200\n",
      "100/100 [==============================] - 0s 191us/sample - loss: 0.0703 - accuracy: 0.9600\n",
      "Epoch 150/200\n",
      "100/100 [==============================] - 0s 291us/sample - loss: 0.0700 - accuracy: 0.9500\n",
      "Epoch 151/200\n",
      "100/100 [==============================] - 0s 227us/sample - loss: 0.0716 - accuracy: 0.9600\n",
      "Epoch 152/200\n",
      "100/100 [==============================] - 0s 273us/sample - loss: 0.0707 - accuracy: 0.9500\n",
      "Epoch 153/200\n",
      "100/100 [==============================] - 0s 237us/sample - loss: 0.0706 - accuracy: 0.9500\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 198us/sample - loss: 0.0697 - accuracy: 0.9500\n",
      "Epoch 155/200\n",
      "100/100 [==============================] - 0s 258us/sample - loss: 0.0682 - accuracy: 0.9600\n",
      "Epoch 156/200\n",
      "100/100 [==============================] - 0s 229us/sample - loss: 0.0692 - accuracy: 0.9500\n",
      "Epoch 157/200\n",
      "100/100 [==============================] - 0s 202us/sample - loss: 0.0687 - accuracy: 0.9600\n",
      "Epoch 158/200\n",
      "100/100 [==============================] - 0s 193us/sample - loss: 0.0690 - accuracy: 0.9600\n",
      "Epoch 159/200\n",
      "100/100 [==============================] - 0s 216us/sample - loss: 0.0709 - accuracy: 0.9500\n",
      "Epoch 160/200\n",
      "100/100 [==============================] - 0s 203us/sample - loss: 0.0680 - accuracy: 0.9600\n",
      "Epoch 161/200\n",
      "100/100 [==============================] - 0s 214us/sample - loss: 0.0680 - accuracy: 0.9600\n",
      "Epoch 162/200\n",
      "100/100 [==============================] - 0s 219us/sample - loss: 0.0701 - accuracy: 0.9400\n",
      "Epoch 163/200\n",
      "100/100 [==============================] - 0s 181us/sample - loss: 0.0692 - accuracy: 0.9600\n",
      "Epoch 164/200\n",
      "100/100 [==============================] - 0s 220us/sample - loss: 0.0673 - accuracy: 0.9400\n",
      "Epoch 165/200\n",
      "100/100 [==============================] - 0s 332us/sample - loss: 0.0670 - accuracy: 0.9500\n",
      "Epoch 166/200\n",
      "100/100 [==============================] - 0s 345us/sample - loss: 0.0662 - accuracy: 0.9500\n",
      "Epoch 167/200\n",
      "100/100 [==============================] - 0s 200us/sample - loss: 0.0658 - accuracy: 0.9600\n",
      "Epoch 168/200\n",
      "100/100 [==============================] - 0s 262us/sample - loss: 0.0680 - accuracy: 0.9500\n",
      "Epoch 169/200\n",
      "100/100 [==============================] - 0s 228us/sample - loss: 0.0651 - accuracy: 0.9500\n",
      "Epoch 170/200\n",
      "100/100 [==============================] - 0s 207us/sample - loss: 0.0649 - accuracy: 0.9600\n",
      "Epoch 171/200\n",
      "100/100 [==============================] - 0s 211us/sample - loss: 0.0646 - accuracy: 0.9500\n",
      "Epoch 172/200\n",
      "100/100 [==============================] - 0s 207us/sample - loss: 0.0676 - accuracy: 0.9600\n",
      "Epoch 173/200\n",
      "100/100 [==============================] - 0s 191us/sample - loss: 0.0660 - accuracy: 0.9600\n",
      "Epoch 174/200\n",
      "100/100 [==============================] - 0s 213us/sample - loss: 0.0659 - accuracy: 0.9700\n",
      "Epoch 175/200\n",
      "100/100 [==============================] - 0s 177us/sample - loss: 0.0636 - accuracy: 0.9600\n",
      "Epoch 176/200\n",
      "100/100 [==============================] - 0s 280us/sample - loss: 0.0642 - accuracy: 0.9700\n",
      "Epoch 177/200\n",
      "100/100 [==============================] - 0s 308us/sample - loss: 0.0647 - accuracy: 0.9600\n",
      "Epoch 178/200\n",
      "100/100 [==============================] - 0s 400us/sample - loss: 0.0633 - accuracy: 0.9700\n",
      "Epoch 179/200\n",
      "100/100 [==============================] - 0s 233us/sample - loss: 0.0635 - accuracy: 0.9500\n",
      "Epoch 180/200\n",
      "100/100 [==============================] - 0s 264us/sample - loss: 0.0634 - accuracy: 0.9700\n",
      "Epoch 181/200\n",
      "100/100 [==============================] - 0s 206us/sample - loss: 0.0620 - accuracy: 0.9700\n",
      "Epoch 182/200\n",
      "100/100 [==============================] - 0s 192us/sample - loss: 0.0633 - accuracy: 0.9600\n",
      "Epoch 183/200\n",
      "100/100 [==============================] - 0s 228us/sample - loss: 0.0630 - accuracy: 0.9600\n",
      "Epoch 184/200\n",
      "100/100 [==============================] - 0s 191us/sample - loss: 0.0646 - accuracy: 0.9500\n",
      "Epoch 185/200\n",
      "100/100 [==============================] - 0s 162us/sample - loss: 0.0609 - accuracy: 0.9700\n",
      "Epoch 186/200\n",
      "100/100 [==============================] - 0s 187us/sample - loss: 0.0609 - accuracy: 0.9700\n",
      "Epoch 187/200\n",
      "100/100 [==============================] - 0s 216us/sample - loss: 0.0612 - accuracy: 0.9700\n",
      "Epoch 188/200\n",
      "100/100 [==============================] - 0s 208us/sample - loss: 0.0611 - accuracy: 0.9600\n",
      "Epoch 189/200\n",
      "100/100 [==============================] - 0s 209us/sample - loss: 0.0600 - accuracy: 0.9600\n",
      "Epoch 190/200\n",
      "100/100 [==============================] - 0s 287us/sample - loss: 0.0613 - accuracy: 0.9600\n",
      "Epoch 191/200\n",
      "100/100 [==============================] - 0s 268us/sample - loss: 0.0609 - accuracy: 0.9700\n",
      "Epoch 192/200\n",
      "100/100 [==============================] - 0s 283us/sample - loss: 0.0610 - accuracy: 0.9600\n",
      "Epoch 193/200\n",
      "100/100 [==============================] - 0s 248us/sample - loss: 0.0605 - accuracy: 0.9600\n",
      "Epoch 194/200\n",
      "100/100 [==============================] - 0s 218us/sample - loss: 0.0608 - accuracy: 0.9600\n",
      "Epoch 195/200\n",
      "100/100 [==============================] - 0s 214us/sample - loss: 0.0611 - accuracy: 0.9500\n",
      "Epoch 196/200\n",
      "100/100 [==============================] - 0s 214us/sample - loss: 0.0611 - accuracy: 0.9700\n",
      "Epoch 197/200\n",
      "100/100 [==============================] - 0s 199us/sample - loss: 0.0609 - accuracy: 0.9500\n",
      "Epoch 198/200\n",
      "100/100 [==============================] - 0s 197us/sample - loss: 0.0600 - accuracy: 0.9600\n",
      "Epoch 199/200\n",
      "100/100 [==============================] - 0s 207us/sample - loss: 0.0622 - accuracy: 0.9600\n",
      "Epoch 200/200\n",
      "100/100 [==============================] - 0s 206us/sample - loss: 0.0589 - accuracy: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff315d614e0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_claim_evidence_concat, y[:100], epochs=200, batch_size=10, validation_data=(val_claim_evidence_concat, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhVg9ZLymOUc"
   },
   "source": [
    "# A simple extension\n",
    "\n",
    "Lastly, we ask you to modify previously defined neural architectures by adding an additional feature to the classification input.\n",
    "\n",
    "We would like to see if some similarity information between the claim to verify and one of its associated evidence might be useful to the classification.\n",
    "\n",
    "Compute the cosine similarity metric between the two sentence embeddings and concatenate the result to the classification input.\n",
    "\n",
    "For clarity, since the cosine similarity of two vectors outputs a scalar value, the classification input shape is modified as follows:\n",
    "\n",
    "*     **Concatenation**: [batch_size, 2 * embedding_dim + 1]\n",
    "\n",
    "*     **Sum**: [batch_size, embedding_dim + 1]\n",
    "\n",
    "*     **Mean**: [batch_size, embedding_dim + 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd74ULgpnJrc"
   },
   "source": [
    "# Performance evaluation\n",
    "\n",
    "Due to our simplifications, obtained results are not directly compatible with a traditional fact checking method that considers the evidence set as a whole.\n",
    "\n",
    "Thus, we need to consider two types of evaluations.\n",
    "\n",
    "**Multi-input classification evaluation**\n",
    "\n",
    "This type of evaluation is the easiest and concerns computing evaluation metrics, such as accuracy, f1-score, recall and precision, of our pre-processed dataset.\n",
    "\n",
    "In other words, we assess the performance of chosen classifiers.\n",
    "\n",
    "**Claim verification evaluation**\n",
    "\n",
    "However, if we want to give an answer concerning the claim itself, we need to consider the whole evidence set. \n",
    "\n",
    "Intuitively, for a given claim, we consider all its corresponding (claim, evidence) pairs and their corresponding classification outputs. \n",
    "\n",
    "At this point, all we need to do is to compute the final predicted claim label via majority voting.\n",
    "\n",
    "Example:\n",
    "\n",
    "    Claim: c1\n",
    "    Evidence set: e1, e2, e3\n",
    "    True label: S\n",
    "\n",
    "    Pair outputs:\n",
    "    (c1, e1) -> S (supports)\n",
    "    (c1, e2) -> S (supports)\n",
    "    (c1, e3) -> R (refutes)\n",
    "\n",
    "    Majority voting:\n",
    "    S -> 2 votes\n",
    "    R -> 1 vote\n",
    "\n",
    "    Final label:\n",
    "    c1 -> S\n",
    "\n",
    "Lastly, we have to compute classification metrics just like before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4LJ2yPxsUOV"
   },
   "source": [
    "# Tips and Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf80UVRNrXve"
   },
   "source": [
    "## Extensions are welcome!\n",
    "\n",
    "Is this task too easy for you? Are you curious to try out things you have seen during lectures (e.g. attention)? Feel free to try everything you want!\n",
    "\n",
    "Don't forget to try neural baselines first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COXeCXdYsBEf"
   },
   "source": [
    "## Comments and documentation\n",
    "\n",
    "Remember to properly comment your code (it is not necessary to comment each single line) and don't forget to describe your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejv6SDE8xc4_"
   },
   "source": [
    "## Organization\n",
    "\n",
    "We suggest you to divide your work into sections. This allows you to build clean and modular code, as well as easy to read and to debug.\n",
    "\n",
    "A possible schema:\n",
    "\n",
    "*   Dataset pre-processing\n",
    "*   Dataset conversion\n",
    "*   Model definition\n",
    "*   Training\n",
    "*   Evaluation\n",
    "*   Comments/Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DR70uh7pabo"
   },
   "source": [
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Andrea Galassi -> a.galassi@unibo.it\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it\n",
    "\n",
    "Don't forget that your feedback is very important! Your suggestions help us improving course material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc0gNWU2pgKQ"
   },
   "source": [
    "# FAQ\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Can I do something text pre-processing?**\n",
    "\n",
    "**A:** You have to! If you check text data, the majority of sentences need some cleaning.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: I'm struggling with the implementation. Can you help me?**\n",
    "\n",
    "**A:** Yes sure! Write us an email about your issue. If you are looking for a particular type of operation, you can easily check the documentation of the deep learning framework you are using (google is your friend).\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Can I try other encoding strategies or neural architectures?**\n",
    "\n",
    "**A:** Absolutely! Remember to try out recommended neural baselines first and only then proceed with your extensions.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment 4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
